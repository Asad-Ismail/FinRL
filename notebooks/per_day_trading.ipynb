{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.2' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/pyfolio/pos.py:26: UserWarning: Module \"zipline.assets\" not found; multipliers will not be applied to position notionals.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "# matplotlib.use('Agg')\n",
    "import datetime\n",
    "\n",
    "%matplotlib inline\n",
    "from finrl.config_tickers import DOW_30_TICKER\n",
    "from finrl.meta.preprocessor.yahoodownloader import YahooDownloader\n",
    "from finrl.meta.preprocessor.preprocessors import FeatureEngineer, data_split\n",
    "from finrl.meta.env_stock_trading.env_stocktrading import StockTradingEnv\n",
    "from finrl.agents.stablebaselines3.models import DRLAgent,DRLEnsembleAgent\n",
    "from finrl.plot import backtest_stats, backtest_plot, get_daily_return, get_baseline\n",
    "\n",
    "from pprint import pprint\n",
    "import sys\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finrl import config\n",
    "from finrl import config_tickers\n",
    "import os\n",
    "from finrl.main import check_and_make_directories\n",
    "from finrl.config import (\n",
    "    DATA_SAVE_DIR,\n",
    "    TRAINED_MODEL_DIR,\n",
    "    TENSORBOARD_LOG_DIR,\n",
    "    RESULTS_DIR,\n",
    "    INDICATORS,\n",
    "    TRAIN_START_DATE,\n",
    "    TRAIN_END_DATE,\n",
    "    TEST_START_DATE,\n",
    "    TEST_END_DATE,\n",
    "    TRADE_START_DATE,\n",
    "    TRADE_END_DATE,\n",
    ")\n",
    "TRAINED_MODEL_DIR=\"best_trained_model\"\n",
    "check_and_make_directories([DATA_SAVE_DIR, TRAINED_MODEL_DIR, TENSORBOARD_LOG_DIR, RESULTS_DIR])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'best_trained_model'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAINED_MODEL_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AXP', 'AMGN', 'AAPL', 'BA', 'CAT', 'CSCO', 'CVX', 'GS', 'HD', 'HON', 'IBM', 'INTC', 'JNJ', 'KO', 'JPM', 'MCD', 'MMM', 'MRK', 'MSFT', 'NKE', 'PG', 'TRV', 'UNH', 'CRM', 'VZ', 'V', 'WBA', 'WMT', 'DIS', 'DOW']\n"
     ]
    }
   ],
   "source": [
    "print(DOW_30_TICKER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finrl.meta.env_stock_trading.env_stocktrading import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockTradingEnv2(gym.Env):\n",
    "    \"\"\"A stock trading environment for OpenAI gym\"\"\"\n",
    "    metadata = {\"render.modes\": [\"human\"]}\n",
    "    def __init__(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        stock_dim: int,\n",
    "        hmax: int,\n",
    "        initial_amount: int,\n",
    "        num_stock_shares: list[int],\n",
    "        buy_cost_pct: list[float],\n",
    "        sell_cost_pct: list[float],\n",
    "        reward_scaling: float,\n",
    "        state_space: int,\n",
    "        action_space: int,\n",
    "        tech_indicator_list: list[str],\n",
    "        turbulence_threshold=None,\n",
    "        risk_indicator_col=\"turbulence\",\n",
    "        make_plots: bool = False,\n",
    "        print_verbosity=10,\n",
    "        day=0,\n",
    "        initial=True,\n",
    "        previous_state=[],\n",
    "        model_name=\"\",\n",
    "        mode=\"\",\n",
    "        iteration=\"\",\n",
    "        random_day=None,\n",
    "        reset_interval=None,\n",
    "    ):\n",
    "        self.day = day\n",
    "        self.df = df\n",
    "        self.stock_dim = stock_dim\n",
    "        self.hmax = hmax\n",
    "        self.num_stock_shares = num_stock_shares\n",
    "        self.initial_amount = initial_amount  # get the initial cash\n",
    "        self.buy_cost_pct = buy_cost_pct\n",
    "        self.sell_cost_pct = sell_cost_pct\n",
    "        self.reward_scaling = reward_scaling\n",
    "        self.state_space = state_space\n",
    "        self.action_space = action_space\n",
    "        self.tech_indicator_list = tech_indicator_list\n",
    "        self.action_space = spaces.Box(low=-1, high=1, shape=(self.action_space,))\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(self.state_space,))\n",
    "        self.data = self.df.loc[self.day, :]\n",
    "        self.terminal = False\n",
    "        self.make_plots = make_plots\n",
    "        self.print_verbosity = print_verbosity\n",
    "        self.turbulence_threshold = turbulence_threshold\n",
    "        self.risk_indicator_col = risk_indicator_col\n",
    "        self.initial = initial\n",
    "        self.previous_state = previous_state\n",
    "        self.model_name = model_name\n",
    "        self.mode = mode\n",
    "        self.iteration = iteration\n",
    "        self.random_day=random_day\n",
    "        self.reset_day=reset_interval\n",
    "        self.reset_interval=reset_interval\n",
    "        # initalize state\n",
    "        self.state = self._initiate_state()\n",
    "\n",
    "        # initialize reward\n",
    "        self.reward = 0\n",
    "        self.turbulence = 0\n",
    "        self.cost = 0\n",
    "        self.trades = 0\n",
    "        self.episode = 0\n",
    "        # memorize all the total balance change\n",
    "        self.asset_memory = [\n",
    "            self.initial_amount\n",
    "            + np.sum(\n",
    "                np.array(self.num_stock_shares)\n",
    "                * np.array(self.state[1 : 1 + self.stock_dim])\n",
    "            )\n",
    "        ]  # the initial total asset is calculated by cash + sum (num_share_stock_i * price_stock_i)\n",
    "        self.rewards_memory = []\n",
    "        self.actions_memory = []\n",
    "        self.state_memory = (\n",
    "            []\n",
    "        )  # we need sometimes to preserve the state in the middle of trading process\n",
    "        self.date_memory = [self._get_date()]\n",
    "        #         self.logger = Logger('results',[CSVOutputFormat])\n",
    "        # self.reset()\n",
    "        self._seed()\n",
    "        print(f\"Intialize Environment StartDay: {self.day}, ResetDay: {self.reset_day},Episode: {self.episode}\")\n",
    "\n",
    "    def _sell_stock(self, index, action):\n",
    "        def _do_sell_normal():\n",
    "            if (self.state[index + 2 * self.stock_dim + 1] != True):\n",
    "                # check if the stock is able to sell, for simlicity we just add it in techical index\n",
    "                # if self.state[index + 1] > 0: # if we use price<0 to denote a stock is unable to trade in that day, the total asset calculation may be wrong for the price is unreasonable\n",
    "                # Sell only if the price is > 0 (no missing data in this particular date)\n",
    "                # perform sell action based on the sign of the action\n",
    "                if self.state[index + self.stock_dim + 1] > 0:\n",
    "                    # Sell only if current asset is > 0\n",
    "                    sell_num_shares = min(abs(action), self.state[index + self.stock_dim + 1])\n",
    "                    sell_amount = (self.state[index + 1]* sell_num_shares* (1 - self.sell_cost_pct[index]))\n",
    "                    # update balance\n",
    "                    self.state[0] += sell_amount\n",
    "\n",
    "                    self.state[index + self.stock_dim + 1] -= sell_num_shares\n",
    "                    self.cost += (self.state[index + 1]* sell_num_shares* self.sell_cost_pct[index])\n",
    "                    self.trades += 1\n",
    "                else:\n",
    "                    sell_num_shares = 0\n",
    "            else:\n",
    "                sell_num_shares = 0\n",
    "\n",
    "            return sell_num_shares\n",
    "\n",
    "        # perform sell action based on the sign of the action\n",
    "        if self.turbulence_threshold is not None:\n",
    "            if self.turbulence >= self.turbulence_threshold:\n",
    "                if self.state[index + 1] > 0:\n",
    "                    # Sell only if the price is > 0 (no missing data in this particular date)\n",
    "                    # if turbulence goes over threshold, just clear out all positions\n",
    "                    if self.state[index + self.stock_dim + 1] > 0:\n",
    "                        # Sell only if current asset is > 0\n",
    "                        sell_num_shares = self.state[index + self.stock_dim + 1]\n",
    "                        sell_amount = (self.state[index + 1]* sell_num_shares* (1 - self.sell_cost_pct[index]))\n",
    "                        # update balance\n",
    "                        self.state[0] += sell_amount\n",
    "                        self.state[index + self.stock_dim + 1] = 0\n",
    "                        self.cost += (\n",
    "                            self.state[index + 1]\n",
    "                            * sell_num_shares\n",
    "                            * self.sell_cost_pct[index]\n",
    "                        )\n",
    "                        self.trades += 1\n",
    "                    else:\n",
    "                        sell_num_shares = 0\n",
    "                else:\n",
    "                    sell_num_shares = 0\n",
    "            else:\n",
    "                sell_num_shares = _do_sell_normal()\n",
    "        else:\n",
    "            sell_num_shares = _do_sell_normal()\n",
    "\n",
    "        return sell_num_shares\n",
    "\n",
    "    def _buy_stock(self, index, action):\n",
    "        def _do_buy():\n",
    "            if (self.state[index + 2 * self.stock_dim + 1] != True):  # check if the stock is able to buy\n",
    "                # if self.state[index + 1] >0:\n",
    "                # Buy only if the price is > 0 (no missing data in this particular date)\n",
    "                available_amount = self.state[0] // (\n",
    "                    self.state[index + 1] * (1 + self.buy_cost_pct[index])\n",
    "                )  # when buying stocks, we should consider the cost of trading when calculating available_amount, or we may be have cash<0\n",
    "                # print('available_amount:{}'.format(available_amount))\n",
    "\n",
    "                # update balance\n",
    "                buy_num_shares = min(available_amount, action)\n",
    "                buy_amount = (self.state[index + 1]* buy_num_shares* (1 + self.buy_cost_pct[index]))\n",
    "                self.state[0] -= buy_amount\n",
    "\n",
    "                self.state[index + self.stock_dim + 1] += buy_num_shares\n",
    "\n",
    "                self.cost += (self.state[index + 1] * buy_num_shares * self.buy_cost_pct[index])\n",
    "                self.trades += 1\n",
    "            else:\n",
    "                buy_num_shares = 0\n",
    "\n",
    "            return buy_num_shares\n",
    "\n",
    "        # perform buy action based on the sign of the action\n",
    "        if self.turbulence_threshold is None:\n",
    "            buy_num_shares = _do_buy()\n",
    "        else:\n",
    "            if self.turbulence < self.turbulence_threshold:\n",
    "                buy_num_shares = _do_buy()\n",
    "            else:\n",
    "                buy_num_shares = 0\n",
    "                pass\n",
    "\n",
    "        return buy_num_shares\n",
    "\n",
    "    def _make_plot(self):\n",
    "        plt.plot(self.asset_memory, \"r\")\n",
    "        plt.savefig(f\"results/account_value_trade_{self.episode}.png\")\n",
    "        plt.close()\n",
    "\n",
    "    def step(self, actions):\n",
    "        \n",
    "        if self.reset_day:\n",
    "            self.terminal = self.day >= self.reset_day\n",
    "            if self.terminal:\n",
    "                print(f\"Environment reached Terminal state as number of trading days reached limit!!\")\n",
    "        else:\n",
    "            self.terminal = self.day >= len(self.df.index.unique()) - 1\n",
    "            print(f\"Environment reached Terminal state as number of trading days reached data limit!!\")\n",
    "            \n",
    "        if self.terminal:\n",
    "            # print(f\"Episode: {self.episode}\")\n",
    "            if self.make_plots:\n",
    "                self._make_plot()\n",
    "            end_total_asset = self.state[0] + sum(\n",
    "                np.array(self.state[1 : (self.stock_dim + 1)])\n",
    "                * np.array(self.state[(self.stock_dim + 1) : (self.stock_dim * 2 + 1)])\n",
    "            )\n",
    "            df_total_value = pd.DataFrame(self.asset_memory)\n",
    "            tot_reward = ( self.state[0] + sum(\n",
    "                    np.array(self.state[1 : (self.stock_dim + 1)])\n",
    "                    * np.array(\n",
    "                        self.state[(self.stock_dim + 1) : (self.stock_dim * 2 + 1)]\n",
    "                    )\n",
    "                )\n",
    "                - self.asset_memory[0]\n",
    "            )  # initial_amount is only cash part of our initial asset\n",
    "            df_total_value.columns = [\"account_value\"]\n",
    "            df_total_value[\"date\"] = self.date_memory\n",
    "            df_total_value[\"daily_return\"] = df_total_value[\"account_value\"].pct_change(1)\n",
    "            if df_total_value[\"daily_return\"].std() != 0:\n",
    "                sharpe = ((252**0.5)* df_total_value[\"daily_return\"].mean()/ df_total_value[\"daily_return\"].std())\n",
    "            df_rewards = pd.DataFrame(self.rewards_memory)\n",
    "            df_rewards.columns = [\"account_rewards\"]\n",
    "            df_rewards[\"date\"] = self.date_memory[:-1]\n",
    "            if self.episode % self.print_verbosity == 0:\n",
    "                print(f\"day: {self.day}, episode: {self.episode}\")\n",
    "                print(f\"begin_total_asset: {self.asset_memory[0]:0.2f}\")\n",
    "                print(f\"end_total_asset: {end_total_asset:0.2f}\")\n",
    "                print(f\"total_reward: {tot_reward:0.2f}\")\n",
    "                print(f\"total_cost: {self.cost:0.2f}\")\n",
    "                print(f\"total_trades: {self.trades}\")\n",
    "                if df_total_value[\"daily_return\"].std() != 0:\n",
    "                    print(f\"Sharpe: {sharpe:0.3f}\")\n",
    "                print(\"=================================\")\n",
    "\n",
    "            if (self.model_name != \"\") and (self.mode != \"\"):\n",
    "                df_actions = self.save_action_memory()\n",
    "                df_actions.to_csv(\n",
    "                    \"results/actions_{}_{}_{}.csv\".format(\n",
    "                        self.mode, self.model_name, self.iteration\n",
    "                    )\n",
    "                )\n",
    "                df_total_value.to_csv(\n",
    "                    \"results/account_value_{}_{}_{}.csv\".format(\n",
    "                        self.mode, self.model_name, self.iteration\n",
    "                    ),\n",
    "                    index=False,\n",
    "                )\n",
    "                df_rewards.to_csv(\n",
    "                    \"results/account_rewards_{}_{}_{}.csv\".format(\n",
    "                        self.mode, self.model_name, self.iteration\n",
    "                    ),\n",
    "                    index=False,\n",
    "                )\n",
    "                plt.plot(self.asset_memory, \"r\")\n",
    "                plt.savefig(\n",
    "                    \"results/account_value_{}_{}_{}.png\".format(\n",
    "                        self.mode, self.model_name, self.iteration\n",
    "                    )\n",
    "                )\n",
    "                plt.close()\n",
    "\n",
    "            return self.state, self.reward, self.terminal, {}\n",
    "\n",
    "        else:\n",
    "            actions = actions * self.hmax  # actions initially is scaled between 0 to 1\n",
    "            actions = actions.astype(\n",
    "                int\n",
    "            )  # convert into integer because we can't by fraction of shares\n",
    "            if self.turbulence_threshold is not None:\n",
    "                if self.turbulence >= self.turbulence_threshold:\n",
    "                    actions = np.array([-self.hmax] * self.stock_dim)\n",
    "            begin_total_asset = self.state[0] + sum(\n",
    "                np.array(self.state[1 : (self.stock_dim + 1)])\n",
    "                * np.array(self.state[(self.stock_dim + 1) : (self.stock_dim * 2 + 1)])\n",
    "            )\n",
    "            # print(\"begin_total_asset:{}\".format(begin_total_asset))\n",
    "\n",
    "            argsort_actions = np.argsort(actions)\n",
    "            sell_index = argsort_actions[: np.where(actions < 0)[0].shape[0]]\n",
    "            buy_index = argsort_actions[::-1][: np.where(actions > 0)[0].shape[0]]\n",
    "\n",
    "            for index in sell_index:\n",
    "                # print(f\"Num shares before: {self.state[index+self.stock_dim+1]}\")\n",
    "                # print(f'take sell action before : {actions[index]}')\n",
    "                actions[index] = self._sell_stock(index, actions[index]) * (-1)\n",
    "                # print(f'take sell action after : {actions[index]}')\n",
    "                # print(f\"Num shares after: {self.state[index+self.stock_dim+1]}\")\n",
    "\n",
    "            for index in buy_index:\n",
    "                # print('take buy action: {}'.format(actions[index]))\n",
    "                actions[index] = self._buy_stock(index, actions[index])\n",
    "\n",
    "            self.actions_memory.append(actions)\n",
    "\n",
    "            # state: s -> s+1\n",
    "            #print(f\"Trading Day {self.day}\")\n",
    "            self.day += 1\n",
    "            self.data = self.df.loc[self.day, :]\n",
    "            if self.turbulence_threshold is not None:\n",
    "                if len(self.df.tic.unique()) == 1:\n",
    "                    self.turbulence = self.data[self.risk_indicator_col]\n",
    "                elif len(self.df.tic.unique()) > 1:\n",
    "                    self.turbulence = self.data[self.risk_indicator_col].values[0]\n",
    "            self.state = self._update_state()\n",
    "\n",
    "            end_total_asset = self.state[0] + sum(\n",
    "                np.array(self.state[1 : (self.stock_dim + 1)])\n",
    "                * np.array(self.state[(self.stock_dim + 1) : (self.stock_dim * 2 + 1)])\n",
    "            )\n",
    "            self.asset_memory.append(end_total_asset)\n",
    "            self.date_memory.append(self._get_date())\n",
    "            self.reward = end_total_asset - begin_total_asset\n",
    "            self.rewards_memory.append(self.reward)\n",
    "            self.reward = self.reward * self.reward_scaling\n",
    "            self.state_memory.append(self.state)  # add current state in state_recorder for each step\n",
    "\n",
    "        return self.state, self.reward, self.terminal, {}\n",
    "\n",
    "    def reset(self):\n",
    "        # initiate state\n",
    "        self.state = self._initiate_state()\n",
    "\n",
    "        if self.initial:\n",
    "            self.asset_memory = [\n",
    "                self.initial_amount\n",
    "                + np.sum(\n",
    "                    np.array(self.num_stock_shares)\n",
    "                    * np.array(self.state[1 : 1 + self.stock_dim])\n",
    "                )\n",
    "            ]\n",
    "        else:\n",
    "            previous_total_asset = self.previous_state[0] + sum(\n",
    "                np.array(self.state[1 : (self.stock_dim + 1)])\n",
    "                * np.array(\n",
    "                    self.previous_state[(self.stock_dim + 1) : (self.stock_dim * 2 + 1)]\n",
    "                )\n",
    "            )\n",
    "            self.asset_memory = [previous_total_asset]\n",
    "            \n",
    "        ## Choose a random day for start of trading    \n",
    "        if self.random_day:\n",
    "            self.day = random.randint(0,len(self.df.index.unique())-self.reset_interval-1)\n",
    "            if self.reset_day:\n",
    "                self.reset_day=self.day+self.reset_interval\n",
    "        else:\n",
    "            self.day = 0\n",
    "            \n",
    "        self.data = self.df.loc[self.day, :]\n",
    "        self.turbulence = 0\n",
    "        self.cost = 0\n",
    "        self.trades = 0\n",
    "        self.terminal = False\n",
    "        # self.iteration=self.iteration\n",
    "        self.rewards_memory = []\n",
    "        self.actions_memory = []\n",
    "        self.date_memory = [self._get_date()]\n",
    "\n",
    "        self.episode += 1\n",
    "        print(f\"Reseting Environment StartDay: {self.day}, ResetDay: {self.reset_day},Episode: {self.episode}\")\n",
    "\n",
    "        return self.state\n",
    "\n",
    "    def render(self, mode=\"human\", close=False):\n",
    "        return self.state\n",
    "\n",
    "    def _initiate_state(self):\n",
    "        if self.initial:\n",
    "            # For Initial State\n",
    "            if len(self.df.tic.unique()) > 1:\n",
    "                # for multiple stock\n",
    "                state = (\n",
    "                    [self.initial_amount]\n",
    "                    + self.data.close.values.tolist()\n",
    "                    + self.num_stock_shares\n",
    "                    + sum(\n",
    "                        (\n",
    "                            self.data[tech].values.tolist()\n",
    "                            for tech in self.tech_indicator_list\n",
    "                        ),\n",
    "                        [],\n",
    "                    )\n",
    "                )  # append initial stocks_share to initial state, instead of all zero\n",
    "            else:\n",
    "                # for single stock\n",
    "                state = (\n",
    "                    [self.initial_amount]\n",
    "                    + [self.data.close]\n",
    "                    + [0] * self.stock_dim\n",
    "                    + sum(([self.data[tech]] for tech in self.tech_indicator_list), [])\n",
    "                )\n",
    "        else:\n",
    "            # Using Previous State\n",
    "            if len(self.df.tic.unique()) > 1:\n",
    "                # for multiple stock\n",
    "                state = (\n",
    "                    [self.previous_state[0]]\n",
    "                    + self.data.close.values.tolist()\n",
    "                    + self.previous_state[\n",
    "                        (self.stock_dim + 1) : (self.stock_dim * 2 + 1)\n",
    "                    ]\n",
    "                    + sum(\n",
    "                        (\n",
    "                            self.data[tech].values.tolist()\n",
    "                            for tech in self.tech_indicator_list\n",
    "                        ),\n",
    "                        [],\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                # for single stock\n",
    "                state = (\n",
    "                    [self.previous_state[0]]\n",
    "                    + [self.data.close]\n",
    "                    + self.previous_state[\n",
    "                        (self.stock_dim + 1) : (self.stock_dim * 2 + 1)\n",
    "                    ]\n",
    "                    + sum(([self.data[tech]] for tech in self.tech_indicator_list), [])\n",
    "                )\n",
    "        return state\n",
    "\n",
    "    def _update_state(self):\n",
    "        if len(self.df.tic.unique()) > 1:\n",
    "            # for multiple stock\n",
    "            state = (\n",
    "                [self.state[0]]\n",
    "                + self.data.close.values.tolist()\n",
    "                + list(self.state[(self.stock_dim + 1) : (self.stock_dim * 2 + 1)])\n",
    "                + sum(\n",
    "                    (\n",
    "                        self.data[tech].values.tolist()\n",
    "                        for tech in self.tech_indicator_list\n",
    "                    ),\n",
    "                    [],\n",
    "                )\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            # for single stock\n",
    "            state = (\n",
    "                [self.state[0]]\n",
    "                + [self.data.close]\n",
    "                + list(self.state[(self.stock_dim + 1) : (self.stock_dim * 2 + 1)])\n",
    "                + sum(([self.data[tech]] for tech in self.tech_indicator_list), [])\n",
    "            )\n",
    "\n",
    "        return state\n",
    "\n",
    "    def _get_date(self):\n",
    "        if len(self.df.tic.unique()) > 1:\n",
    "            date = self.data.date.unique()[0]\n",
    "        else:\n",
    "            date = self.data.date\n",
    "        return date\n",
    "\n",
    "    # add save_state_memory to preserve state in the trading process\n",
    "    def save_state_memory(self):\n",
    "        if len(self.df.tic.unique()) > 1:\n",
    "            # date and close price length must match actions length\n",
    "            date_list = self.date_memory[:-1]\n",
    "            df_date = pd.DataFrame(date_list)\n",
    "            df_date.columns = [\"date\"]\n",
    "\n",
    "            state_list = self.state_memory\n",
    "            df_states = pd.DataFrame(\n",
    "                state_list,\n",
    "                columns=[\n",
    "                    \"cash\",\n",
    "                    \"Bitcoin_price\",\n",
    "                    \"Gold_price\",\n",
    "                    \"Bitcoin_num\",\n",
    "                    \"Gold_num\",\n",
    "                    \"Bitcoin_Disable\",\n",
    "                    \"Gold_Disable\",\n",
    "                ],\n",
    "            )\n",
    "            df_states.index = df_date.date\n",
    "            # df_actions = pd.DataFrame({'date':date_list,'actions':action_list})\n",
    "        else:\n",
    "            date_list = self.date_memory[:-1]\n",
    "            state_list = self.state_memory\n",
    "            df_states = pd.DataFrame({\"date\": date_list, \"states\": state_list})\n",
    "        # print(df_states)\n",
    "        return df_states\n",
    "\n",
    "    def save_asset_memory(self):\n",
    "        date_list = self.date_memory\n",
    "        asset_list = self.asset_memory\n",
    "        df_account_value = pd.DataFrame(\n",
    "            {\"date\": date_list, \"account_value\": asset_list}\n",
    "        )\n",
    "        return df_account_value\n",
    "\n",
    "    def save_action_memory(self):\n",
    "        if len(self.df.tic.unique()) > 1:\n",
    "            # date and close price length must match actions length\n",
    "            date_list = self.date_memory[:-1]\n",
    "            df_date = pd.DataFrame(date_list)\n",
    "            df_date.columns = [\"date\"]\n",
    "\n",
    "            action_list = self.actions_memory\n",
    "            df_actions = pd.DataFrame(action_list)\n",
    "            df_actions.columns = self.data.tic.values\n",
    "            df_actions.index = df_date.date\n",
    "            # df_actions = pd.DataFrame({'date':date_list,'actions':action_list})\n",
    "        else:\n",
    "            date_list = self.date_memory[:-1]\n",
    "            action_list = self.actions_memory\n",
    "            df_actions = pd.DataFrame({\"date\": date_list, \"actions\": action_list})\n",
    "        return df_actions\n",
    "\n",
    "    def _seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def get_sb_env(self):\n",
    "        e = DummyVecEnv([lambda: self])\n",
    "        obs = e.reset()\n",
    "        return e, obs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "def get_nth_previous_date(n):\n",
    "    today = datetime.today()\n",
    "    nth_previous_date = today - timedelta(days=n)\n",
    "    return nth_previous_date.strftime('%Y-%m-%d')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2023-02-11'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#validate on last 10 days\n",
    "n=10\n",
    "TRAIN_END_DATE=TEST_START_DATE=get_nth_previous_date(n)\n",
    "TRAIN_END_DATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/yahoodownloader.py:53: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_df = data_df.append(temp_df)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/yahoodownloader.py:53: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_df = data_df.append(temp_df)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/yahoodownloader.py:53: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_df = data_df.append(temp_df)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/yahoodownloader.py:53: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_df = data_df.append(temp_df)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/yahoodownloader.py:53: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_df = data_df.append(temp_df)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/yahoodownloader.py:53: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_df = data_df.append(temp_df)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/yahoodownloader.py:53: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_df = data_df.append(temp_df)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/yahoodownloader.py:53: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_df = data_df.append(temp_df)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/yahoodownloader.py:53: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_df = data_df.append(temp_df)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/yahoodownloader.py:53: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_df = data_df.append(temp_df)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/yahoodownloader.py:53: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_df = data_df.append(temp_df)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/yahoodownloader.py:53: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_df = data_df.append(temp_df)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/yahoodownloader.py:53: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_df = data_df.append(temp_df)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/yahoodownloader.py:53: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_df = data_df.append(temp_df)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/yahoodownloader.py:53: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_df = data_df.append(temp_df)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/yahoodownloader.py:53: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_df = data_df.append(temp_df)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/yahoodownloader.py:53: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_df = data_df.append(temp_df)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/yahoodownloader.py:53: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_df = data_df.append(temp_df)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/yahoodownloader.py:53: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_df = data_df.append(temp_df)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/yahoodownloader.py:53: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_df = data_df.append(temp_df)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/yahoodownloader.py:53: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_df = data_df.append(temp_df)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/yahoodownloader.py:53: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_df = data_df.append(temp_df)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/yahoodownloader.py:53: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_df = data_df.append(temp_df)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/yahoodownloader.py:53: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_df = data_df.append(temp_df)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/yahoodownloader.py:53: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_df = data_df.append(temp_df)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/yahoodownloader.py:53: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_df = data_df.append(temp_df)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/yahoodownloader.py:53: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_df = data_df.append(temp_df)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/yahoodownloader.py:53: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_df = data_df.append(temp_df)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/yahoodownloader.py:53: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_df = data_df.append(temp_df)\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/yahoodownloader.py:53: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_df = data_df.append(temp_df)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DataFrame:  (130859, 8)\n"
     ]
    }
   ],
   "source": [
    "TRAIN_START_DATE = '2005-04-01'\n",
    "# Test date is something unreachable in this lifetime\n",
    "TEST_END_DATE = '2080-02-16'\n",
    "df = YahooDownloader(start_date = TRAIN_START_DATE,\n",
    "                     end_date = TEST_END_DATE,\n",
    "                     ticker_list = DOW_30_TICKER).fetch_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(130859, 8)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/finrl/meta/preprocessor/preprocessors.py:158: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  indicator_df = indicator_df.append(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully added technical indicators\n",
      "Successfully added turbulence index\n"
     ]
    }
   ],
   "source": [
    "fe = FeatureEngineer(use_technical_indicator=True,\n",
    "                     tech_indicator_list = INDICATORS,\n",
    "                     use_turbulence=True,\n",
    "                     user_defined_feature = False)\n",
    "\n",
    "processed = fe.preprocess_data(df)\n",
    "processed = processed.copy()\n",
    "processed = processed.fillna(0)\n",
    "processed = processed.replace(np.inf,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>tic</th>\n",
       "      <th>day</th>\n",
       "      <th>macd</th>\n",
       "      <th>boll_ub</th>\n",
       "      <th>boll_lb</th>\n",
       "      <th>rsi_30</th>\n",
       "      <th>cci_30</th>\n",
       "      <th>dx_30</th>\n",
       "      <th>close_30_sma</th>\n",
       "      <th>close_60_sma</th>\n",
       "      <th>turbulence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>126107</th>\n",
       "      <td>2023-02-21</td>\n",
       "      <td>185.419998</td>\n",
       "      <td>186.360001</td>\n",
       "      <td>184.139999</td>\n",
       "      <td>184.320206</td>\n",
       "      <td>250569</td>\n",
       "      <td>TRV</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.809061</td>\n",
       "      <td>193.277733</td>\n",
       "      <td>180.543288</td>\n",
       "      <td>49.359314</td>\n",
       "      <td>-56.720870</td>\n",
       "      <td>10.885409</td>\n",
       "      <td>187.348340</td>\n",
       "      <td>187.351487</td>\n",
       "      <td>21.08147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126108</th>\n",
       "      <td>2023-02-21</td>\n",
       "      <td>496.700012</td>\n",
       "      <td>501.394989</td>\n",
       "      <td>492.140015</td>\n",
       "      <td>495.309998</td>\n",
       "      <td>1014660</td>\n",
       "      <td>UNH</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.465698</td>\n",
       "      <td>505.582597</td>\n",
       "      <td>470.812401</td>\n",
       "      <td>47.960983</td>\n",
       "      <td>91.820457</td>\n",
       "      <td>8.519067</td>\n",
       "      <td>487.895332</td>\n",
       "      <td>508.449635</td>\n",
       "      <td>21.08147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126109</th>\n",
       "      <td>2023-02-21</td>\n",
       "      <td>39.900002</td>\n",
       "      <td>39.980000</td>\n",
       "      <td>39.400002</td>\n",
       "      <td>39.465000</td>\n",
       "      <td>5721088</td>\n",
       "      <td>VZ</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.019596</td>\n",
       "      <td>41.942736</td>\n",
       "      <td>39.260764</td>\n",
       "      <td>48.820859</td>\n",
       "      <td>-103.957206</td>\n",
       "      <td>35.771198</td>\n",
       "      <td>40.690500</td>\n",
       "      <td>39.283687</td>\n",
       "      <td>21.08147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126110</th>\n",
       "      <td>2023-02-21</td>\n",
       "      <td>36.380001</td>\n",
       "      <td>36.509998</td>\n",
       "      <td>36.090000</td>\n",
       "      <td>36.349998</td>\n",
       "      <td>1835368</td>\n",
       "      <td>WBA</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.069970</td>\n",
       "      <td>37.181454</td>\n",
       "      <td>35.348825</td>\n",
       "      <td>48.743560</td>\n",
       "      <td>-11.082192</td>\n",
       "      <td>14.338233</td>\n",
       "      <td>36.135497</td>\n",
       "      <td>37.582818</td>\n",
       "      <td>21.08147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126111</th>\n",
       "      <td>2023-02-21</td>\n",
       "      <td>142.880005</td>\n",
       "      <td>148.339996</td>\n",
       "      <td>142.149994</td>\n",
       "      <td>147.273193</td>\n",
       "      <td>10743539</td>\n",
       "      <td>WMT</td>\n",
       "      <td>1</td>\n",
       "      <td>0.687985</td>\n",
       "      <td>147.640967</td>\n",
       "      <td>139.355354</td>\n",
       "      <td>55.034568</td>\n",
       "      <td>84.898881</td>\n",
       "      <td>25.315200</td>\n",
       "      <td>143.442107</td>\n",
       "      <td>145.154809</td>\n",
       "      <td>21.08147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              date        open        high         low       close    volume  \\\n",
       "126107  2023-02-21  185.419998  186.360001  184.139999  184.320206    250569   \n",
       "126108  2023-02-21  496.700012  501.394989  492.140015  495.309998   1014660   \n",
       "126109  2023-02-21   39.900002   39.980000   39.400002   39.465000   5721088   \n",
       "126110  2023-02-21   36.380001   36.509998   36.090000   36.349998   1835368   \n",
       "126111  2023-02-21  142.880005  148.339996  142.149994  147.273193  10743539   \n",
       "\n",
       "        tic  day      macd     boll_ub     boll_lb     rsi_30      cci_30  \\\n",
       "126107  TRV    1 -0.809061  193.277733  180.543288  49.359314  -56.720870   \n",
       "126108  UNH    1 -1.465698  505.582597  470.812401  47.960983   91.820457   \n",
       "126109   VZ    1 -0.019596   41.942736   39.260764  48.820859 -103.957206   \n",
       "126110  WBA    1 -0.069970   37.181454   35.348825  48.743560  -11.082192   \n",
       "126111  WMT    1  0.687985  147.640967  139.355354  55.034568   84.898881   \n",
       "\n",
       "            dx_30  close_30_sma  close_60_sma  turbulence  \n",
       "126107  10.885409    187.348340    187.351487    21.08147  \n",
       "126108   8.519067    487.895332    508.449635    21.08147  \n",
       "126109  35.771198     40.690500     39.283687    21.08147  \n",
       "126110  14.338233     36.135497     37.582818    21.08147  \n",
       "126111  25.315200    143.442107    145.154809    21.08147  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(126112, 17)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(126112, 4504, 28)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(processed.index.unique()),len(processed[\"date\"].unique()),len(processed[processed[\"date\"]==\"2023-02-17\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'DOW', 'V'},\n",
       " ['AAPL',\n",
       "  'AMGN',\n",
       "  'AXP',\n",
       "  'BA',\n",
       "  'CAT',\n",
       "  'CRM',\n",
       "  'CSCO',\n",
       "  'CVX',\n",
       "  'DIS',\n",
       "  'GS',\n",
       "  'HD',\n",
       "  'HON',\n",
       "  'IBM',\n",
       "  'INTC',\n",
       "  'JNJ',\n",
       "  'JPM',\n",
       "  'KO',\n",
       "  'MCD',\n",
       "  'MMM',\n",
       "  'MRK',\n",
       "  'MSFT',\n",
       "  'NKE',\n",
       "  'PG',\n",
       "  'TRV',\n",
       "  'UNH',\n",
       "  'VZ',\n",
       "  'WBA',\n",
       "  'WMT'])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "available_ticks=list(processed.tic.unique())\n",
    "set(DOW_30_TICKER)-set(available_ticks),available_ticks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stock Dimension: 28, State Space: 281\n"
     ]
    }
   ],
   "source": [
    "stock_dimension = len(processed.tic.unique())\n",
    "state_space = 1 + 2*stock_dimension + len(INDICATORS)*stock_dimension\n",
    "print(f\"Stock Dimension: {stock_dimension}, State Space: {state_space}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_kwargs = {\n",
    "    \"hmax\": 5, \n",
    "    \"initial_amount\": 500, \n",
    "    \"buy_cost_pct\": 0.01, \n",
    "    \"sell_cost_pct\": 0.01, \n",
    "    \"state_space\": state_space, \n",
    "    \"stock_dim\": stock_dimension, \n",
    "    \"tech_indicator_list\": INDICATORS,\n",
    "    \"action_space\": stock_dimension, \n",
    "    \"reward_scaling\": 1e-4,\n",
    "    \"print_verbosity\":5\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "A2C_model_kwargs = {\n",
    "                    'n_steps': 5,\n",
    "                    'ent_coef': 0.005,\n",
    "                    'learning_rate': 0.0007\n",
    "                    }\n",
    "\n",
    "PPO_model_kwargs = {\n",
    "                    \"ent_coef\":0.01,\n",
    "                    \"n_steps\": 2048,\n",
    "                    \"learning_rate\": 0.00025,\n",
    "                    \"batch_size\": 128\n",
    "                    }\n",
    "\n",
    "DDPG_model_kwargs = {\n",
    "                      #\"action_noise\":\"ornstein_uhlenbeck\",\n",
    "                      \"buffer_size\": 10_000,\n",
    "                      \"learning_rate\": 0.0005,\n",
    "                      \"batch_size\": 64\n",
    "                    }\n",
    "\n",
    "timesteps_dict = {'a2c' : 10_0000, \n",
    "                 'ppo' : 10_00000, \n",
    "                 'ddpg' : 10_0000\n",
    "                 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finrl.agents.stablebaselines3.models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DRLAgent:\n",
    "    @staticmethod\n",
    "    def get_model(\n",
    "        model_name,\n",
    "        env,\n",
    "        policy=\"MlpPolicy\",\n",
    "        policy_kwargs=None,\n",
    "        model_kwargs=None,\n",
    "        seed=None,\n",
    "        verbose=1,\n",
    "    ):\n",
    "        if model_name not in MODELS:\n",
    "            raise NotImplementedError(\"NotImplementedError\")\n",
    "\n",
    "        if model_kwargs is None:\n",
    "            temp_model_kwargs = MODEL_KWARGS[model_name]\n",
    "        else:\n",
    "            temp_model_kwargs = model_kwargs.copy()\n",
    "\n",
    "        if \"action_noise\" in temp_model_kwargs:\n",
    "            n_actions = env.action_space.shape[-1]\n",
    "            temp_model_kwargs[\"action_noise\"] = NOISE[\n",
    "                temp_model_kwargs[\"action_noise\"]\n",
    "            ](mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))\n",
    "        print(temp_model_kwargs)\n",
    "        return MODELS[model_name](\n",
    "            policy=policy,\n",
    "            env=env,\n",
    "            tensorboard_log=f\"{config.TENSORBOARD_LOG_DIR}/{model_name}\",\n",
    "            verbose=verbose,\n",
    "            policy_kwargs=policy_kwargs,\n",
    "            seed=seed,\n",
    "            **temp_model_kwargs,\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def train_model(model, model_name, tb_log_name, iter_num, total_timesteps=5000):\n",
    "        model = model.learn(\n",
    "            total_timesteps=total_timesteps,\n",
    "            tb_log_name=tb_log_name,\n",
    "            callback=TensorboardCallback(),\n",
    "        )\n",
    "        model.save(\n",
    "            f\"{config.TRAINED_MODEL_DIR}/{model_name.upper()}_{total_timesteps // 1000}k_{iter_num}\"\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def get_validation_sharpe(iteration, model_name):\n",
    "        \"\"\"Calculate Sharpe ratio based on validation results\"\"\"\n",
    "        df_total_value = pd.read_csv(\n",
    "            f\"results/account_value_validation_{model_name}_{iteration}.csv\"\n",
    "        )\n",
    "        # If the agent did not make any transaction\n",
    "        if df_total_value[\"daily_return\"].var() == 0:\n",
    "            if df_total_value[\"daily_return\"].mean() > 0:\n",
    "                return np.inf\n",
    "            else:\n",
    "                return 0.0\n",
    "        else:\n",
    "            return (\n",
    "                (4**0.5)\n",
    "                * df_total_value[\"daily_return\"].mean()\n",
    "                / df_total_value[\"daily_return\"].std()\n",
    "            )\n",
    "    @staticmethod\n",
    "    def get_modelWeights(model,path):\n",
    "        file_list = os.listdir(path)\n",
    "        sorted_list = sorted([f for f in file_list if f.lower().startswith(model)])\n",
    "        if not sorted_list:\n",
    "            raise(\"Pretrained weights not Found!!\")\n",
    "        else:\n",
    "            print(f\"Found weights {sorted_list[-1]}!!\")\n",
    "        return sorted_list[-1]\n",
    "        \n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        df,\n",
    "        train_period,\n",
    "        val_test_period,\n",
    "        stock_dim,\n",
    "        hmax,\n",
    "        initial_amount,\n",
    "        buy_cost_pct,\n",
    "        sell_cost_pct,\n",
    "        reward_scaling,\n",
    "        state_space,\n",
    "        action_space,\n",
    "        tech_indicator_list,\n",
    "        print_verbosity,\n",
    "        use_pretrain=False,\n",
    "        pretrain_pth=\"\",\n",
    "        best_model=\"ppo\",\n",
    "        reset_interval=60\n",
    "        \n",
    "    ):\n",
    "        self.df = df\n",
    "        self.train_period = train_period\n",
    "        self.val_test_period = val_test_period\n",
    "        self.unique_trade_date = df[(df.date > val_test_period[0]) & (df.date <= val_test_period[1])].date.unique()\n",
    "        self.stock_dim = stock_dim\n",
    "        self.hmax = hmax\n",
    "        self.initial_amount = initial_amount\n",
    "        self.buy_cost_pct = buy_cost_pct\n",
    "        self.sell_cost_pct = sell_cost_pct\n",
    "        self.reward_scaling = reward_scaling\n",
    "        self.state_space = state_space\n",
    "        self.action_space = action_space\n",
    "        self.tech_indicator_list = tech_indicator_list\n",
    "        self.print_verbosity = print_verbosity\n",
    "        self.use_pretrain=use_pretrain\n",
    "        self.pretrain_pth=pretrain_pth\n",
    "        self.best_model=best_model\n",
    "        self.reset_interval=reset_interval\n",
    "\n",
    "    def DRL_validation(self, model, test_data, test_env, test_obs):\n",
    "        \"\"\"validation process\"\"\"\n",
    "        for _ in range(len(test_data.index.unique())):\n",
    "            action, _states = model.predict(test_obs)\n",
    "            test_obs, rewards, dones, info = test_env.step(action)\n",
    "\n",
    "    def DRL_prediction(\n",
    "        self, model, name, last_state, iter_num, turbulence_threshold, initial\n",
    "    ):\n",
    "        \"\"\"make a prediction based on trained model\"\"\"\n",
    "        \n",
    "        ## trading env\n",
    "        trade_data = data_split(\n",
    "            self.df,\n",
    "            start=self.unique_trade_date[0],\n",
    "            end=self.unique_trade_date[-1],\n",
    "        )\n",
    "        trade_env = DummyVecEnv(\n",
    "            [\n",
    "                lambda: StockTradingEnv(\n",
    "                    df=trade_data,\n",
    "                    stock_dim=self.stock_dim,\n",
    "                    hmax=self.hmax,\n",
    "                    initial_amount=self.initial_amount,\n",
    "                    num_stock_shares=[0] * self.stock_dim,\n",
    "                    buy_cost_pct=[self.buy_cost_pct] * self.stock_dim,\n",
    "                    sell_cost_pct=[self.sell_cost_pct] * self.stock_dim,\n",
    "                    reward_scaling=self.reward_scaling,\n",
    "                    state_space=self.state_space,\n",
    "                    action_space=self.action_space,\n",
    "                    tech_indicator_list=self.tech_indicator_list,\n",
    "                    turbulence_threshold=turbulence_threshold,\n",
    "                    initial=initial,\n",
    "                    previous_state=last_state,\n",
    "                    model_name=name,\n",
    "                    mode=\"trade\",\n",
    "                    iteration=iter_num,\n",
    "                    print_verbosity=self.print_verbosity,\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        trade_obs = trade_env.reset()\n",
    "\n",
    "        for i in range(len(trade_data.index.unique())):\n",
    "            action, _states = model.predict(trade_obs)\n",
    "            #print(f\"Training actions are {action}\")\n",
    "            trade_obs, rewards, dones, info = trade_env.step(action)\n",
    "            if i == (len(trade_data.index.unique()) - 2):\n",
    "                # print(env_test.render())\n",
    "                last_state = trade_env.render()\n",
    "\n",
    "        df_last_state = pd.DataFrame({\"last_state\": last_state})\n",
    "        df_last_state.to_csv(f\"results/last_state_{name}_{i}.csv\", index=False)\n",
    "        return last_state\n",
    "    \n",
    "    \n",
    "    def DRL_single_prediction(\n",
    "        self, model, name=\"ensemble\", initial=True,last_state=[]\n",
    "    ):\n",
    "        \"\"\"make a prediction based on trained model\"\"\"\n",
    "\n",
    "        ## trading env\n",
    "        trade_data = data_split(\n",
    "            self.df,\n",
    "            start=self.unique_trade_date[-1],\n",
    "            end=\"2080-01-01\",\n",
    "        )\n",
    "        trade_env = DummyVecEnv(\n",
    "            [\n",
    "                lambda: StockTradingEnv(\n",
    "                    df=trade_data,\n",
    "                    stock_dim=self.stock_dim,\n",
    "                    hmax=self.hmax,\n",
    "                    initial_amount=self.initial_amount,\n",
    "                    num_stock_shares=[0] * self.stock_dim,\n",
    "                    buy_cost_pct=[self.buy_cost_pct] * self.stock_dim,\n",
    "                    sell_cost_pct=[self.sell_cost_pct] * self.stock_dim,\n",
    "                    reward_scaling=self.reward_scaling,\n",
    "                    state_space=self.state_space,\n",
    "                    action_space=self.action_space,\n",
    "                    tech_indicator_list=self.tech_indicator_list,\n",
    "                    turbulence_threshold=turbulence_threshold,\n",
    "                    initial=initial,\n",
    "                    previous_state=last_state,\n",
    "                    model_name=name,\n",
    "                    mode=\"trade\",\n",
    "                    print_verbosity=self.print_verbosity,\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        trade_obs = trade_env.reset()\n",
    "        actions=[]\n",
    "\n",
    "        for i in range(len(trade_data.index.unique())):\n",
    "            action, _states = model.predict(trade_obs)\n",
    "            print(f\"Day Action is {action}\")\n",
    "            actions.append(action)\n",
    "\n",
    "        return actions,last_state\n",
    "\n",
    "    def run_strategy(self, A2C_model_kwargs, PPO_model_kwargs, DDPG_model_kwargs, timesteps_dict):\n",
    "        i=1\n",
    "        \"\"\"Ensemble Strategy that combines PPO, A2C and DDPG\"\"\"\n",
    "        print(\"============Training Best Model============\")\n",
    "\n",
    "        insample_turbulence = self.df[(self.df.date < self.train_period[1])& (self.df.date >= self.train_period[0])]\n",
    "        insample_turbulence_threshold = np.quantile(insample_turbulence.turbulence.values, 0.90)\n",
    "        \n",
    "        historical_turbulence = insample_turbulence.drop_duplicates(subset=[\"date\"])\n",
    "        historical_turbulence_mean = np.mean(historical_turbulence.turbulence.values)\n",
    "        \n",
    "        if historical_turbulence_mean > insample_turbulence_threshold:\n",
    "            # if the mean of the historical data is greater than the 90% quantile of insample turbulence data\n",
    "            # then we assume that the current market is volatile,\n",
    "            # therefore we set the 90% quantile of insample turbulence data as the turbulence threshold\n",
    "            # meaning the current turbulence can't exceed the 90% quantile of insample turbulence data\n",
    "            turbulence_threshold = insample_turbulence_threshold\n",
    "        else:\n",
    "            # if the mean of the historical data is less than the 90% quantile of insample turbulence data\n",
    "            # then we tune up the turbulence_threshold, meaning we lower the risk\n",
    "            turbulence_threshold = np.quantile(\n",
    "                insample_turbulence.turbulence.values, 1\n",
    "            )\n",
    "\n",
    "        #turbulence_threshold = np.quantile(insample_turbulence.turbulence.values, 0.99)\n",
    "        \n",
    "        print(\"Turbulence_threshold: \", turbulence_threshold)\n",
    "\n",
    "        self.turbulence_threshold=turbulence_threshold\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        ############## Environment Setup starts ##############\n",
    "        ## training env\n",
    "        train = data_split(self.df,start=self.train_period[0],end=self.train_period[1],)\n",
    "        stday = random.randint(0,len(train.index.unique())-(self.reset_interval*self.stock_dim)-2)\n",
    "        self.train_env = DummyVecEnv(\n",
    "            [\n",
    "                lambda: StockTradingEnv2(\n",
    "                    df=train,\n",
    "                    stock_dim=self.stock_dim,\n",
    "                    hmax=self.hmax,\n",
    "                    initial_amount=self.initial_amount,\n",
    "                    num_stock_shares=[0] * self.stock_dim,\n",
    "                    buy_cost_pct=[self.buy_cost_pct] * self.stock_dim,\n",
    "                    sell_cost_pct=[self.sell_cost_pct] * self.stock_dim,\n",
    "                    reward_scaling=self.reward_scaling,\n",
    "                    state_space=self.state_space,\n",
    "                    action_space=self.action_space,\n",
    "                    tech_indicator_list=self.tech_indicator_list,\n",
    "                    print_verbosity=self.print_verbosity,\n",
    "                    random_day=stday,\n",
    "                    reset_interval=self.reset_interval\n",
    "                )\n",
    "            ]\n",
    "            )\n",
    "        validation = data_split(self.df,start=self.unique_trade_date[0],end=self.unique_trade_date[-1],)\n",
    "        val_env = DummyVecEnv(\n",
    "                [\n",
    "                    lambda: StockTradingEnv(\n",
    "                        df=validation,\n",
    "                        stock_dim=self.stock_dim,\n",
    "                        hmax=self.hmax,\n",
    "                        initial_amount=self.initial_amount,\n",
    "                        num_stock_shares=[0] * self.stock_dim,\n",
    "                        buy_cost_pct=[self.buy_cost_pct] * self.stock_dim,\n",
    "                        sell_cost_pct=[self.sell_cost_pct] * self.stock_dim,\n",
    "                        reward_scaling=self.reward_scaling,\n",
    "                        state_space=self.state_space,\n",
    "                        action_space=self.action_space,\n",
    "                        tech_indicator_list=self.tech_indicator_list,\n",
    "                        turbulence_threshold=turbulence_threshold,\n",
    "                        iteration=1,\n",
    "                        model_name=\"BestModel\",\n",
    "                        mode=\"validation\",\n",
    "                        print_verbosity=self.print_verbosity,\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "        val_obs = val_env.reset()\n",
    "        ############## Environment Setup ends ##############\n",
    "        ############## Training and Validation starts ##############\n",
    "        print(\n",
    "            \"======Model training from: \",\n",
    "            self.train_period[0],\n",
    "            \"to \",\n",
    "            self.train_period[1]\n",
    "        )\n",
    "        if self.best_model==\"a2c\":\n",
    "            model_a2c = self.get_model(\"a2c\", self.train_env, policy=\"MlpPolicy\", model_kwargs=A2C_model_kwargs)\n",
    "            if self.use_pretrain:\n",
    "                print(\"======Loading A2C Pretrained Model========\")\n",
    "                model_a2c.load(os.path.join(self.pretrain_pth,self.get_modelWeights(\"a2c\",self.pretrain_pth)))\n",
    "            print(\"======A2C Training========\")\n",
    "            model_a2c = self.train_model(\n",
    "                model_a2c,\n",
    "                \"a2c\",\n",
    "                tb_log_name=f\"a2c_{i}\",\n",
    "                iter_num=i,\n",
    "                total_timesteps=timesteps_dict[\"a2c\"],\n",
    "            )  \n",
    "            print(\"======A2C Validation from: \",\n",
    "                validation_start_date,\n",
    "                \"to \",\n",
    "                validation_end_date,)\n",
    "            self.DRL_validation(\n",
    "                model=model_a2c,\n",
    "                test_data=validation,\n",
    "                test_env=val_env,\n",
    "                test_obs=val_obs,\n",
    "            )\n",
    "            sharpe_a2c = self.get_validation_sharpe(1, model_name=\"BestModel\")\n",
    "            print(\"A2C Sharpe Ratio: \", sharpe_a2c)\n",
    "            self.sharpe=sharpe_a2c\n",
    "            self.model=model_a2c\n",
    "            \n",
    "        elif self.best_model==\"ppo\":\n",
    "            \n",
    "            model_ppo = self.get_model(\"ppo\", self.train_env, policy=\"MlpPolicy\", model_kwargs=PPO_model_kwargs)\n",
    "            if self.use_pretrain:\n",
    "                print(\"======Loading PPO Pretrained Model========\")\n",
    "                model_ppo.load(os.path.join(self.pretrain_pth,self.get_modelWeights(\"ppo\",self.pretrain_pth)))\n",
    "            print(\"======PPO Training========\")\n",
    "            model_ppo = self.train_model(\n",
    "                model_ppo,\n",
    "                \"ppo\",\n",
    "                tb_log_name=f\"ppo_{i}\",\n",
    "                iter_num=i,\n",
    "                total_timesteps=timesteps_dict[\"ppo\"],\n",
    "            )\n",
    "            print(\n",
    "                \"======PPO Validation from: \",\n",
    "                self.unique_trade_date[0],\n",
    "                \"to \",\n",
    "                self.unique_trade_date[-1],\n",
    "            )\n",
    "            self.DRL_validation(\n",
    "                model=model_ppo,\n",
    "                test_data=validation,\n",
    "                test_env=val_env,\n",
    "                test_obs=val_obs,\n",
    "            )\n",
    "            sharpe_ppo = self.get_validation_sharpe(1, model_name=\"BestModel\")\n",
    "            print(\"PPO Sharpe Ratio: \", sharpe_ppo)\n",
    "            self.sharpe=sharpe_ppo\n",
    "            self.model=model_ppo\n",
    "        \n",
    "        elif self.best_model==\"ddpg\":\n",
    "            \n",
    "            model_ddpg = self.get_model(\n",
    "                \"ddpg\",\n",
    "                self.train_env,\n",
    "                policy=\"MlpPolicy\",\n",
    "                model_kwargs=DDPG_model_kwargs,\n",
    "            )\n",
    "            if self.use_pretrain:\n",
    "                print(\"======Loading DDPG Pretrained Model========\")\n",
    "                model_ddpg.load(os.path.join(self.pretrain_pth,self.get_modelWeights(\"ddpg\",self.pretrain_pth)))\n",
    "            print(\"======DDPG Training========\")\n",
    "            model_ddpg = self.train_model(\n",
    "                model_ddpg,\n",
    "                \"ddpg\",\n",
    "                tb_log_name=f\"ddpg_{i}\",\n",
    "                iter_num=i,\n",
    "                total_timesteps=timesteps_dict[\"ddpg\"],\n",
    "            )  # 50_000\n",
    "            \n",
    "            self.DRL_validation(\n",
    "                model=model_ddpg,\n",
    "                test_data=validation,\n",
    "                test_env=val_env,\n",
    "                test_obs=val_obs,\n",
    "            )\n",
    "            sharpe_ddpg = self.get_validation_sharpe(1, model_name=\"BestModel\")\n",
    "            print(\"DDPg Sharpe Ratio: \", sharpe_ddpg)\n",
    "            self.sharpe=sharpe_ddpg\n",
    "            self.model=model_ddpg\n",
    "            \n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        ############## Trading starts ##############\n",
    "        print(\"======Trading from: \",\n",
    "            self.unique_trade_date[0],\n",
    "            \"to \",\n",
    "            self.unique_trade_date[-1],)\n",
    "        \n",
    "        last_state_ensemble = self.DRL_prediction(\n",
    "            model=self.model,\n",
    "            name=\"BestModel\",\n",
    "            last_state=[],\n",
    "            iter_num=1,\n",
    "            turbulence_threshold=turbulence_threshold,\n",
    "            initial=True,\n",
    "        )\n",
    "        \n",
    "        end = time.time()\n",
    "        print(\"Training Strategy took: \", (end - start) / 60, \" minutes\")\n",
    "        ## Assign the model as final model\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DRLAgent(df=processed,\n",
    "                 train_period=(TRAIN_START_DATE,TRAIN_END_DATE),\n",
    "                 val_test_period=(TEST_START_DATE,TEST_END_DATE),\n",
    "                 use_pretrain=False,\n",
    "                 pretrain_pth=\"/mnt/trained_models\",\n",
    "                 best_model=\"ppo\",\n",
    "                 **env_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============Training Best Model============\n",
      "Turbulence_threshold:  639.3259952984249\n",
      "Intialize Environment StartDay: 0, ResetDay: 1680,Episode: 0\n",
      "======Model training from:  2005-04-01 to  2023-02-07\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cuda device\n",
      "======PPO Training========\n",
      "Reseting Environment StartDay: 1229, ResetDay: 2909,Episode: 1\n",
      "Logging to tensorboard_log/ppo/ppo_1_1\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 1622, ResetDay: 3302,Episode: 2\n",
      "--------------------------------------\n",
      "| time/              |               |\n",
      "|    fps             | 78            |\n",
      "|    iterations      | 1             |\n",
      "|    time_elapsed    | 25            |\n",
      "|    total_timesteps | 2048          |\n",
      "| train/             |               |\n",
      "|    reward          | 5.9591675e-05 |\n",
      "--------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 2480, ResetDay: 4160,Episode: 3\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 76        |\n",
      "|    iterations           | 2         |\n",
      "|    time_elapsed         | 53        |\n",
      "|    total_timesteps      | 4096      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 5598.5713 |\n",
      "|    clip_fraction        | 0.939     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -39.9     |\n",
      "|    explained_variance   | -3.19     |\n",
      "|    learning_rate        | 0.00025   |\n",
      "|    loss                 | -0.0823   |\n",
      "|    n_updates            | 10        |\n",
      "|    policy_gradient_loss | 0.264     |\n",
      "|    reward               | 0.0       |\n",
      "|    std                  | 1.01      |\n",
      "|    value_loss           | 0.0759    |\n",
      "---------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 2625, ResetDay: 4305,Episode: 4\n",
      "--------------------------------------\n",
      "| time/                   |          |\n",
      "|    fps                  | 76       |\n",
      "|    iterations           | 3        |\n",
      "|    time_elapsed         | 80       |\n",
      "|    total_timesteps      | 6144     |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 120.9171 |\n",
      "|    clip_fraction        | 0.679    |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -40.2    |\n",
      "|    explained_variance   | -2.45    |\n",
      "|    learning_rate        | 0.00025  |\n",
      "|    loss                 | -0.299   |\n",
      "|    n_updates            | 20       |\n",
      "|    policy_gradient_loss | 0.15     |\n",
      "|    reward               | 0.0      |\n",
      "|    std                  | 1.02     |\n",
      "|    value_loss           | 0.0153   |\n",
      "--------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 638, ResetDay: 2318,Episode: 5\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 76        |\n",
      "|    iterations           | 4         |\n",
      "|    time_elapsed         | 107       |\n",
      "|    total_timesteps      | 8192      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 115.45049 |\n",
      "|    clip_fraction        | 0.686     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -40.4     |\n",
      "|    explained_variance   | -2.42     |\n",
      "|    learning_rate        | 0.00025   |\n",
      "|    loss                 | -0.3      |\n",
      "|    n_updates            | 30        |\n",
      "|    policy_gradient_loss | 0.173     |\n",
      "|    reward               | 0.0       |\n",
      "|    std                  | 1.03      |\n",
      "|    value_loss           | 0.00682   |\n",
      "---------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "day: 2318, episode: 5\n",
      "begin_total_asset: 500.00\n",
      "end_total_asset: 6.93\n",
      "total_reward: -493.07\n",
      "total_cost: 157.45\n",
      "total_trades: 21178\n",
      "Sharpe: -1.667\n",
      "=================================\n",
      "Reseting Environment StartDay: 1768, ResetDay: 3448,Episode: 6\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 759, ResetDay: 2439,Episode: 7\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 75          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 135         |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 490.13214   |\n",
      "|    clip_fraction        | 0.699       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -40.5       |\n",
      "|    explained_variance   | -5.3        |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.343      |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | 0.108       |\n",
      "|    reward               | 0.000287821 |\n",
      "|    std                  | 1.03        |\n",
      "|    value_loss           | 0.0162      |\n",
      "-----------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 19, ResetDay: 1699,Episode: 8\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 75             |\n",
      "|    iterations           | 6              |\n",
      "|    time_elapsed         | 162            |\n",
      "|    total_timesteps      | 12288          |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 103.42568      |\n",
      "|    clip_fraction        | 0.712          |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -40.6          |\n",
      "|    explained_variance   | -4.41          |\n",
      "|    learning_rate        | 0.00025        |\n",
      "|    loss                 | -0.266         |\n",
      "|    n_updates            | 50             |\n",
      "|    policy_gradient_loss | 0.143          |\n",
      "|    reward               | -4.3326378e-05 |\n",
      "|    std                  | 1.03           |\n",
      "|    value_loss           | 0.00558        |\n",
      "--------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 171, ResetDay: 1851,Episode: 9\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 75           |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 190          |\n",
      "|    total_timesteps      | 14336        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 28.975773    |\n",
      "|    clip_fraction        | 0.7          |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -40.7        |\n",
      "|    explained_variance   | -6.68        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.287       |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | 0.113        |\n",
      "|    reward               | 7.904053e-07 |\n",
      "|    std                  | 1.04         |\n",
      "|    value_loss           | 0.00977      |\n",
      "------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 906, ResetDay: 2586,Episode: 10\n",
      "--------------------------------------\n",
      "| time/                   |          |\n",
      "|    fps                  | 75       |\n",
      "|    iterations           | 8        |\n",
      "|    time_elapsed         | 217      |\n",
      "|    total_timesteps      | 16384    |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 66.03015 |\n",
      "|    clip_fraction        | 0.716    |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -40.9    |\n",
      "|    explained_variance   | -6.47    |\n",
      "|    learning_rate        | 0.00025  |\n",
      "|    loss                 | -0.333   |\n",
      "|    n_updates            | 70       |\n",
      "|    policy_gradient_loss | 0.129    |\n",
      "|    reward               | 0.0      |\n",
      "|    std                  | 1.05     |\n",
      "|    value_loss           | 0.00816  |\n",
      "--------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "day: 2586, episode: 10\n",
      "begin_total_asset: 500.00\n",
      "end_total_asset: 10.59\n",
      "total_reward: -489.41\n",
      "total_cost: 335.37\n",
      "total_trades: 22912\n",
      "Sharpe: -2.004\n",
      "=================================\n",
      "Reseting Environment StartDay: 1427, ResetDay: 3107,Episode: 11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 75        |\n",
      "|    iterations           | 9         |\n",
      "|    time_elapsed         | 245       |\n",
      "|    total_timesteps      | 18432     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 41.663143 |\n",
      "|    clip_fraction        | 0.717     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -41       |\n",
      "|    explained_variance   | -4.31     |\n",
      "|    learning_rate        | 0.00025   |\n",
      "|    loss                 | -0.256    |\n",
      "|    n_updates            | 80        |\n",
      "|    policy_gradient_loss | 0.141     |\n",
      "|    reward               | 0.0       |\n",
      "|    std                  | 1.05      |\n",
      "|    value_loss           | 0.00399   |\n",
      "---------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 1629, ResetDay: 3309,Episode: 12\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 588, ResetDay: 2268,Episode: 13\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 75             |\n",
      "|    iterations           | 10             |\n",
      "|    time_elapsed         | 272            |\n",
      "|    total_timesteps      | 20480          |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 109.08848      |\n",
      "|    clip_fraction        | 0.766          |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -41.1          |\n",
      "|    explained_variance   | -7.1           |\n",
      "|    learning_rate        | 0.00025        |\n",
      "|    loss                 | -0.283         |\n",
      "|    n_updates            | 90             |\n",
      "|    policy_gradient_loss | 0.326          |\n",
      "|    reward               | -0.00021149206 |\n",
      "|    std                  | 1.05           |\n",
      "|    value_loss           | 0.00256        |\n",
      "--------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 149, ResetDay: 1829,Episode: 14\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 75        |\n",
      "|    iterations           | 11        |\n",
      "|    time_elapsed         | 300       |\n",
      "|    total_timesteps      | 22528     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 233.68951 |\n",
      "|    clip_fraction        | 0.751     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -41.2     |\n",
      "|    explained_variance   | -6.4      |\n",
      "|    learning_rate        | 0.00025   |\n",
      "|    loss                 | -0.24     |\n",
      "|    n_updates            | 100       |\n",
      "|    policy_gradient_loss | 0.168     |\n",
      "|    reward               | 0.0       |\n",
      "|    std                  | 1.06      |\n",
      "|    value_loss           | 0.00225   |\n",
      "---------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 133, ResetDay: 1813,Episode: 15\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 12        |\n",
      "|    time_elapsed         | 327       |\n",
      "|    total_timesteps      | 24576     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 29.689615 |\n",
      "|    clip_fraction        | 0.74      |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -41.3     |\n",
      "|    explained_variance   | -7.33     |\n",
      "|    learning_rate        | 0.00025   |\n",
      "|    loss                 | -0.308    |\n",
      "|    n_updates            | 110       |\n",
      "|    policy_gradient_loss | 0.211     |\n",
      "|    reward               | 0.0       |\n",
      "|    std                  | 1.06      |\n",
      "|    value_loss           | 0.0035    |\n",
      "---------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "day: 1813, episode: 15\n",
      "begin_total_asset: 500.00\n",
      "end_total_asset: 5.56\n",
      "total_reward: -494.44\n",
      "total_cost: 354.24\n",
      "total_trades: 24292\n",
      "Sharpe: -2.496\n",
      "=================================\n",
      "Reseting Environment StartDay: 2533, ResetDay: 4213,Episode: 16\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 13            |\n",
      "|    time_elapsed         | 355           |\n",
      "|    total_timesteps      | 26624         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 47.674843     |\n",
      "|    clip_fraction        | 0.742         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -41.4         |\n",
      "|    explained_variance   | -7.23         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.355        |\n",
      "|    n_updates            | 120           |\n",
      "|    policy_gradient_loss | 0.141         |\n",
      "|    reward               | 1.4973415e-05 |\n",
      "|    std                  | 1.06          |\n",
      "|    value_loss           | 0.00278       |\n",
      "-------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 1040, ResetDay: 2720,Episode: 17\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 406, ResetDay: 2086,Episode: 18\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 14            |\n",
      "|    time_elapsed         | 382           |\n",
      "|    total_timesteps      | 28672         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 51.981888     |\n",
      "|    clip_fraction        | 0.773         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -41.5         |\n",
      "|    explained_variance   | -6.28         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.309        |\n",
      "|    n_updates            | 130           |\n",
      "|    policy_gradient_loss | 0.154         |\n",
      "|    reward               | -0.0004059719 |\n",
      "|    std                  | 1.07          |\n",
      "|    value_loss           | 0.00129       |\n",
      "-------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 1928, ResetDay: 3608,Episode: 19\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 74             |\n",
      "|    iterations           | 15             |\n",
      "|    time_elapsed         | 409            |\n",
      "|    total_timesteps      | 30720          |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 551.8362       |\n",
      "|    clip_fraction        | 0.755          |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -41.6          |\n",
      "|    explained_variance   | -7.68          |\n",
      "|    learning_rate        | 0.00025        |\n",
      "|    loss                 | -0.31          |\n",
      "|    n_updates            | 140            |\n",
      "|    policy_gradient_loss | 0.137          |\n",
      "|    reward               | -0.00026405786 |\n",
      "|    std                  | 1.07           |\n",
      "|    value_loss           | 0.00135        |\n",
      "--------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 2630, ResetDay: 4310,Episode: 20\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 16            |\n",
      "|    time_elapsed         | 436           |\n",
      "|    total_timesteps      | 32768         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 55.74517      |\n",
      "|    clip_fraction        | 0.729         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -41.8         |\n",
      "|    explained_variance   | -6.56         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.346        |\n",
      "|    n_updates            | 150           |\n",
      "|    policy_gradient_loss | 0.137         |\n",
      "|    reward               | 3.8658523e-05 |\n",
      "|    std                  | 1.08          |\n",
      "|    value_loss           | 0.00171       |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "day: 4310, episode: 20\n",
      "begin_total_asset: 500.00\n",
      "end_total_asset: 31.37\n",
      "total_reward: -468.63\n",
      "total_cost: 384.10\n",
      "total_trades: 27612\n",
      "Sharpe: -2.043\n",
      "=================================\n",
      "Reseting Environment StartDay: 2615, ResetDay: 4295,Episode: 21\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 75             |\n",
      "|    iterations           | 17             |\n",
      "|    time_elapsed         | 464            |\n",
      "|    total_timesteps      | 34816          |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 123.51064      |\n",
      "|    clip_fraction        | 0.751          |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -41.9          |\n",
      "|    explained_variance   | -10.2          |\n",
      "|    learning_rate        | 0.00025        |\n",
      "|    loss                 | -0.291         |\n",
      "|    n_updates            | 160            |\n",
      "|    policy_gradient_loss | 0.146          |\n",
      "|    reward               | -3.4620665e-05 |\n",
      "|    std                  | 1.08           |\n",
      "|    value_loss           | 0.000787       |\n",
      "--------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 2584, ResetDay: 4264,Episode: 22\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 18        |\n",
      "|    time_elapsed         | 491       |\n",
      "|    total_timesteps      | 36864     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 342.96014 |\n",
      "|    clip_fraction        | 0.778     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -42       |\n",
      "|    explained_variance   | -8.6      |\n",
      "|    learning_rate        | 0.00025   |\n",
      "|    loss                 | -0.304    |\n",
      "|    n_updates            | 170       |\n",
      "|    policy_gradient_loss | 0.208     |\n",
      "|    reward               | 0.0       |\n",
      "|    std                  | 1.09      |\n",
      "|    value_loss           | 0.000485  |\n",
      "---------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 1024, ResetDay: 2704,Episode: 23\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 1954, ResetDay: 3634,Episode: 24\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 19            |\n",
      "|    time_elapsed         | 519           |\n",
      "|    total_timesteps      | 38912         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 380.47623     |\n",
      "|    clip_fraction        | 0.745         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -42.2         |\n",
      "|    explained_variance   | -12.3         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.304        |\n",
      "|    n_updates            | 180           |\n",
      "|    policy_gradient_loss | 0.139         |\n",
      "|    reward               | 3.2991695e-05 |\n",
      "|    std                  | 1.09          |\n",
      "|    value_loss           | 0.000371      |\n",
      "-------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 1571, ResetDay: 3251,Episode: 25\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 20            |\n",
      "|    time_elapsed         | 546           |\n",
      "|    total_timesteps      | 40960         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 748.0671      |\n",
      "|    clip_fraction        | 0.757         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -42.3         |\n",
      "|    explained_variance   | -7.66         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.332        |\n",
      "|    n_updates            | 190           |\n",
      "|    policy_gradient_loss | 0.165         |\n",
      "|    reward               | 4.2666627e-05 |\n",
      "|    std                  | 1.1           |\n",
      "|    value_loss           | 0.00104       |\n",
      "-------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "day: 3251, episode: 25\n",
      "begin_total_asset: 500.00\n",
      "end_total_asset: 17.72\n",
      "total_reward: -482.28\n",
      "total_cost: 162.03\n",
      "total_trades: 31036\n",
      "Sharpe: -1.397\n",
      "=================================\n",
      "Reseting Environment StartDay: 1117, ResetDay: 2797,Episode: 26\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 74             |\n",
      "|    iterations           | 21             |\n",
      "|    time_elapsed         | 573            |\n",
      "|    total_timesteps      | 43008          |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 104.88803      |\n",
      "|    clip_fraction        | 0.757          |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -42.5          |\n",
      "|    explained_variance   | -6.98          |\n",
      "|    learning_rate        | 0.00025        |\n",
      "|    loss                 | -0.289         |\n",
      "|    n_updates            | 200            |\n",
      "|    policy_gradient_loss | 0.485          |\n",
      "|    reward               | -3.7380833e-05 |\n",
      "|    std                  | 1.11           |\n",
      "|    value_loss           | 0.000625       |\n",
      "--------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 2771, ResetDay: 4451,Episode: 27\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 22        |\n",
      "|    time_elapsed         | 601       |\n",
      "|    total_timesteps      | 45056     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 105.02755 |\n",
      "|    clip_fraction        | 0.763     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -42.6     |\n",
      "|    explained_variance   | -9.05     |\n",
      "|    learning_rate        | 0.00025   |\n",
      "|    loss                 | -0.31     |\n",
      "|    n_updates            | 210       |\n",
      "|    policy_gradient_loss | 0.187     |\n",
      "|    reward               | 0.0       |\n",
      "|    std                  | 1.11      |\n",
      "|    value_loss           | 0.000682  |\n",
      "---------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 156, ResetDay: 1836,Episode: 28\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 220, ResetDay: 1900,Episode: 29\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 74             |\n",
      "|    iterations           | 23             |\n",
      "|    time_elapsed         | 628            |\n",
      "|    total_timesteps      | 47104          |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 90.5032        |\n",
      "|    clip_fraction        | 0.733          |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -42.7          |\n",
      "|    explained_variance   | -7.31          |\n",
      "|    learning_rate        | 0.00025        |\n",
      "|    loss                 | -0.343         |\n",
      "|    n_updates            | 220            |\n",
      "|    policy_gradient_loss | 0.17           |\n",
      "|    reward               | -0.00012396017 |\n",
      "|    std                  | 1.12           |\n",
      "|    value_loss           | 0.000359       |\n",
      "--------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 1835, ResetDay: 3515,Episode: 30\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 74             |\n",
      "|    iterations           | 24             |\n",
      "|    time_elapsed         | 656            |\n",
      "|    total_timesteps      | 49152          |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 727.26526      |\n",
      "|    clip_fraction        | 0.734          |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -42.9          |\n",
      "|    explained_variance   | -8.17          |\n",
      "|    learning_rate        | 0.00025        |\n",
      "|    loss                 | -0.256         |\n",
      "|    n_updates            | 230            |\n",
      "|    policy_gradient_loss | 0.132          |\n",
      "|    reward               | -0.00029930114 |\n",
      "|    std                  | 1.12           |\n",
      "|    value_loss           | 0.00128        |\n",
      "--------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "day: 3515, episode: 30\n",
      "begin_total_asset: 500.00\n",
      "end_total_asset: 57.33\n",
      "total_reward: -442.67\n",
      "total_cost: 518.36\n",
      "total_trades: 35105\n",
      "Sharpe: -1.572\n",
      "=================================\n",
      "Reseting Environment StartDay: 1681, ResetDay: 3361,Episode: 31\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 74             |\n",
      "|    iterations           | 25             |\n",
      "|    time_elapsed         | 683            |\n",
      "|    total_timesteps      | 51200          |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 41.57265       |\n",
      "|    clip_fraction        | 0.727          |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -43            |\n",
      "|    explained_variance   | -7.47          |\n",
      "|    learning_rate        | 0.00025        |\n",
      "|    loss                 | -0.296         |\n",
      "|    n_updates            | 240            |\n",
      "|    policy_gradient_loss | 0.127          |\n",
      "|    reward               | -2.3033905e-05 |\n",
      "|    std                  | 1.13           |\n",
      "|    value_loss           | 0.000917       |\n",
      "--------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 723, ResetDay: 2403,Episode: 32\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 74             |\n",
      "|    iterations           | 26             |\n",
      "|    time_elapsed         | 710            |\n",
      "|    total_timesteps      | 53248          |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 103.15979      |\n",
      "|    clip_fraction        | 0.729          |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -43.2          |\n",
      "|    explained_variance   | -7.26          |\n",
      "|    learning_rate        | 0.00025        |\n",
      "|    loss                 | -0.29          |\n",
      "|    n_updates            | 250            |\n",
      "|    policy_gradient_loss | 0.153          |\n",
      "|    reward               | -1.8854522e-05 |\n",
      "|    std                  | 1.13           |\n",
      "|    value_loss           | 0.000419       |\n",
      "--------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 1940, ResetDay: 3620,Episode: 33\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 27            |\n",
      "|    time_elapsed         | 738           |\n",
      "|    total_timesteps      | 55296         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 122.14724     |\n",
      "|    clip_fraction        | 0.729         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -43.3         |\n",
      "|    explained_variance   | -7.33         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.317        |\n",
      "|    n_updates            | 260           |\n",
      "|    policy_gradient_loss | 0.135         |\n",
      "|    reward               | -0.0005418438 |\n",
      "|    std                  | 1.14          |\n",
      "|    value_loss           | 0.000494      |\n",
      "-------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 2310, ResetDay: 3990,Episode: 34\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 2010, ResetDay: 3690,Episode: 35\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 28            |\n",
      "|    time_elapsed         | 765           |\n",
      "|    total_timesteps      | 57344         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 65.1302       |\n",
      "|    clip_fraction        | 0.733         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -43.4         |\n",
      "|    explained_variance   | -10.4         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.3          |\n",
      "|    n_updates            | 270           |\n",
      "|    policy_gradient_loss | 0.158         |\n",
      "|    reward               | 2.4002446e-05 |\n",
      "|    std                  | 1.14          |\n",
      "|    value_loss           | 0.000345      |\n",
      "-------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "day: 3690, episode: 35\n",
      "begin_total_asset: 500.00\n",
      "end_total_asset: 98.75\n",
      "total_reward: -401.25\n",
      "total_cost: 307.69\n",
      "total_trades: 38649\n",
      "Sharpe: -0.705\n",
      "=================================\n",
      "Reseting Environment StartDay: 2601, ResetDay: 4281,Episode: 36\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 29            |\n",
      "|    time_elapsed         | 793           |\n",
      "|    total_timesteps      | 59392         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 291.6929      |\n",
      "|    clip_fraction        | 0.731         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -43.6         |\n",
      "|    explained_variance   | -8.59         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.292        |\n",
      "|    n_updates            | 280           |\n",
      "|    policy_gradient_loss | 0.149         |\n",
      "|    reward               | 3.8543703e-06 |\n",
      "|    std                  | 1.15          |\n",
      "|    value_loss           | 0.000215      |\n",
      "-------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 1861, ResetDay: 3541,Episode: 37\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 74             |\n",
      "|    iterations           | 30             |\n",
      "|    time_elapsed         | 820            |\n",
      "|    total_timesteps      | 61440          |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 99.601715      |\n",
      "|    clip_fraction        | 0.732          |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -43.7          |\n",
      "|    explained_variance   | -15.9          |\n",
      "|    learning_rate        | 0.00025        |\n",
      "|    loss                 | -0.352         |\n",
      "|    n_updates            | 290            |\n",
      "|    policy_gradient_loss | 0.163          |\n",
      "|    reward               | -0.00021546605 |\n",
      "|    std                  | 1.16           |\n",
      "|    value_loss           | 0.00019        |\n",
      "--------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 1427, ResetDay: 3107,Episode: 38\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 31            |\n",
      "|    time_elapsed         | 847           |\n",
      "|    total_timesteps      | 63488         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 251.70074     |\n",
      "|    clip_fraction        | 0.739         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -43.9         |\n",
      "|    explained_variance   | -9.25         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.257        |\n",
      "|    n_updates            | 300           |\n",
      "|    policy_gradient_loss | 0.16          |\n",
      "|    reward               | 0.00014249535 |\n",
      "|    std                  | 1.16          |\n",
      "|    value_loss           | 0.000184      |\n",
      "-------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 2049, ResetDay: 3729,Episode: 39\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 74             |\n",
      "|    iterations           | 32             |\n",
      "|    time_elapsed         | 875            |\n",
      "|    total_timesteps      | 65536          |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 135.47507      |\n",
      "|    clip_fraction        | 0.733          |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -44            |\n",
      "|    explained_variance   | -9.34          |\n",
      "|    learning_rate        | 0.00025        |\n",
      "|    loss                 | -0.257         |\n",
      "|    n_updates            | 310            |\n",
      "|    policy_gradient_loss | 0.169          |\n",
      "|    reward               | -0.00017422027 |\n",
      "|    std                  | 1.17           |\n",
      "|    value_loss           | 0.000258       |\n",
      "--------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 2719, ResetDay: 4399,Episode: 40\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "day: 4399, episode: 40\n",
      "begin_total_asset: 500.00\n",
      "end_total_asset: 336.05\n",
      "total_reward: -163.95\n",
      "total_cost: 242.19\n",
      "total_trades: 41699\n",
      "Sharpe: -0.006\n",
      "=================================\n",
      "Reseting Environment StartDay: 2810, ResetDay: 4490,Episode: 41\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 33            |\n",
      "|    time_elapsed         | 902           |\n",
      "|    total_timesteps      | 67584         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 110.878876    |\n",
      "|    clip_fraction        | 0.733         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -44.2         |\n",
      "|    explained_variance   | -11.7         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.304        |\n",
      "|    n_updates            | 320           |\n",
      "|    policy_gradient_loss | 0.185         |\n",
      "|    reward               | -3.263855e-05 |\n",
      "|    std                  | 1.18          |\n",
      "|    value_loss           | 0.000144      |\n",
      "-------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 1824, ResetDay: 3504,Episode: 42\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 34           |\n",
      "|    time_elapsed         | 929          |\n",
      "|    total_timesteps      | 69632        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 305.38693    |\n",
      "|    clip_fraction        | 0.733        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -44.3        |\n",
      "|    explained_variance   | -6.1         |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.33        |\n",
      "|    n_updates            | 330          |\n",
      "|    policy_gradient_loss | 0.196        |\n",
      "|    reward               | 0.0002013195 |\n",
      "|    std                  | 1.18         |\n",
      "|    value_loss           | 0.000104     |\n",
      "------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 2745, ResetDay: 4425,Episode: 43\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 74             |\n",
      "|    iterations           | 35             |\n",
      "|    time_elapsed         | 957            |\n",
      "|    total_timesteps      | 71680          |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 245.3724       |\n",
      "|    clip_fraction        | 0.716          |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -44.3          |\n",
      "|    explained_variance   | -8             |\n",
      "|    learning_rate        | 0.00025        |\n",
      "|    loss                 | -0.326         |\n",
      "|    n_updates            | 340            |\n",
      "|    policy_gradient_loss | 0.143          |\n",
      "|    reward               | -0.00040277824 |\n",
      "|    std                  | 1.18           |\n",
      "|    value_loss           | 0.000118       |\n",
      "--------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 2682, ResetDay: 4362,Episode: 44\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 36           |\n",
      "|    time_elapsed         | 984          |\n",
      "|    total_timesteps      | 73728        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 126.093445   |\n",
      "|    clip_fraction        | 0.733        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -44.5        |\n",
      "|    explained_variance   | -9.93        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.311       |\n",
      "|    n_updates            | 350          |\n",
      "|    policy_gradient_loss | 0.156        |\n",
      "|    reward               | 0.0006367966 |\n",
      "|    std                  | 1.19         |\n",
      "|    value_loss           | 9.39e-05     |\n",
      "------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 2336, ResetDay: 4016,Episode: 45\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "day: 4016, episode: 45\n",
      "begin_total_asset: 500.00\n",
      "end_total_asset: 450.21\n",
      "total_reward: -49.79\n",
      "total_cost: 97.09\n",
      "total_trades: 43754\n",
      "Sharpe: 0.134\n",
      "=================================\n",
      "Reseting Environment StartDay: 1588, ResetDay: 3268,Episode: 46\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 37            |\n",
      "|    time_elapsed         | 1011          |\n",
      "|    total_timesteps      | 75776         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 325.6227      |\n",
      "|    clip_fraction        | 0.741         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -44.6         |\n",
      "|    explained_variance   | -8            |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.366        |\n",
      "|    n_updates            | 360           |\n",
      "|    policy_gradient_loss | 0.141         |\n",
      "|    reward               | -7.336979e-05 |\n",
      "|    std                  | 1.2           |\n",
      "|    value_loss           | 6.07e-05      |\n",
      "-------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 1480, ResetDay: 3160,Episode: 47\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 38            |\n",
      "|    time_elapsed         | 1039          |\n",
      "|    total_timesteps      | 77824         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 644.9885      |\n",
      "|    clip_fraction        | 0.735         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -44.8         |\n",
      "|    explained_variance   | -8.38         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.358        |\n",
      "|    n_updates            | 370           |\n",
      "|    policy_gradient_loss | 0.176         |\n",
      "|    reward               | 0.00017946816 |\n",
      "|    std                  | 1.2           |\n",
      "|    value_loss           | 0.000105      |\n",
      "-------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 344, ResetDay: 2024,Episode: 48\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 39            |\n",
      "|    time_elapsed         | 1066          |\n",
      "|    total_timesteps      | 79872         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 57.88848      |\n",
      "|    clip_fraction        | 0.75          |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -45           |\n",
      "|    explained_variance   | -7.01         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.324        |\n",
      "|    n_updates            | 380           |\n",
      "|    policy_gradient_loss | 0.161         |\n",
      "|    reward               | -0.0001867239 |\n",
      "|    std                  | 1.21          |\n",
      "|    value_loss           | 0.000197      |\n",
      "-------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 930, ResetDay: 2610,Episode: 49\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 74             |\n",
      "|    iterations           | 40             |\n",
      "|    time_elapsed         | 1094           |\n",
      "|    total_timesteps      | 81920          |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 70.08308       |\n",
      "|    clip_fraction        | 0.737          |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -45.2          |\n",
      "|    explained_variance   | -9.49          |\n",
      "|    learning_rate        | 0.00025        |\n",
      "|    loss                 | -0.315         |\n",
      "|    n_updates            | 390            |\n",
      "|    policy_gradient_loss | 0.125          |\n",
      "|    reward               | -0.00023544102 |\n",
      "|    std                  | 1.22           |\n",
      "|    value_loss           | 0.000511       |\n",
      "--------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 1872, ResetDay: 3552,Episode: 50\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 41           |\n",
      "|    time_elapsed         | 1121         |\n",
      "|    total_timesteps      | 83968        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 40.26688     |\n",
      "|    clip_fraction        | 0.701        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -45.3        |\n",
      "|    explained_variance   | -12.7        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.351       |\n",
      "|    n_updates            | 400          |\n",
      "|    policy_gradient_loss | 0.157        |\n",
      "|    reward               | 0.0006424728 |\n",
      "|    std                  | 1.22         |\n",
      "|    value_loss           | 0.000314     |\n",
      "------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "day: 3552, episode: 50\n",
      "begin_total_asset: 500.00\n",
      "end_total_asset: 835.19\n",
      "total_reward: 335.19\n",
      "total_cost: 27.20\n",
      "total_trades: 45437\n",
      "Sharpe: 0.587\n",
      "=================================\n",
      "Reseting Environment StartDay: 2215, ResetDay: 3895,Episode: 51\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 1266, ResetDay: 2946,Episode: 52\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 74             |\n",
      "|    iterations           | 42             |\n",
      "|    time_elapsed         | 1149           |\n",
      "|    total_timesteps      | 86016          |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 86.34882       |\n",
      "|    clip_fraction        | 0.751          |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -45.4          |\n",
      "|    explained_variance   | -9.43          |\n",
      "|    learning_rate        | 0.00025        |\n",
      "|    loss                 | -0.327         |\n",
      "|    n_updates            | 410            |\n",
      "|    policy_gradient_loss | 0.28           |\n",
      "|    reward               | -0.00011611147 |\n",
      "|    std                  | 1.22           |\n",
      "|    value_loss           | 0.00011        |\n",
      "--------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 1663, ResetDay: 3343,Episode: 53\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 74             |\n",
      "|    iterations           | 43             |\n",
      "|    time_elapsed         | 1176           |\n",
      "|    total_timesteps      | 88064          |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 298.62735      |\n",
      "|    clip_fraction        | 0.771          |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -45.4          |\n",
      "|    explained_variance   | -4.75          |\n",
      "|    learning_rate        | 0.00025        |\n",
      "|    loss                 | -0.291         |\n",
      "|    n_updates            | 420            |\n",
      "|    policy_gradient_loss | 0.337          |\n",
      "|    reward               | -0.00018718338 |\n",
      "|    std                  | 1.23           |\n",
      "|    value_loss           | 9.25e-05       |\n",
      "--------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 1008, ResetDay: 2688,Episode: 54\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 44            |\n",
      "|    time_elapsed         | 1203          |\n",
      "|    total_timesteps      | 90112         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 49.844727     |\n",
      "|    clip_fraction        | 0.691         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -45.5         |\n",
      "|    explained_variance   | -10.8         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.377        |\n",
      "|    n_updates            | 430           |\n",
      "|    policy_gradient_loss | 0.143         |\n",
      "|    reward               | 0.00054610235 |\n",
      "|    std                  | 1.23          |\n",
      "|    value_loss           | 0.00011       |\n",
      "-------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 2760, ResetDay: 4440,Episode: 55\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 45            |\n",
      "|    time_elapsed         | 1231          |\n",
      "|    total_timesteps      | 92160         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 98.510635     |\n",
      "|    clip_fraction        | 0.788         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -45.7         |\n",
      "|    explained_variance   | -6.09         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.314        |\n",
      "|    n_updates            | 440           |\n",
      "|    policy_gradient_loss | 0.215         |\n",
      "|    reward               | -0.0018317756 |\n",
      "|    std                  | 1.24          |\n",
      "|    value_loss           | 0.000126      |\n",
      "-------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "day: 4440, episode: 55\n",
      "begin_total_asset: 500.00\n",
      "end_total_asset: 939.22\n",
      "total_reward: 439.22\n",
      "total_cost: 62.39\n",
      "total_trades: 46245\n",
      "Sharpe: 0.552\n",
      "=================================\n",
      "Reseting Environment StartDay: 481, ResetDay: 2161,Episode: 56\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 1734, ResetDay: 3414,Episode: 57\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 74             |\n",
      "|    iterations           | 46             |\n",
      "|    time_elapsed         | 1258           |\n",
      "|    total_timesteps      | 94208          |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 74.06454       |\n",
      "|    clip_fraction        | 0.725          |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -45.8          |\n",
      "|    explained_variance   | -4.8           |\n",
      "|    learning_rate        | 0.00025        |\n",
      "|    loss                 | -0.31          |\n",
      "|    n_updates            | 450            |\n",
      "|    policy_gradient_loss | 0.172          |\n",
      "|    reward               | -0.00021505203 |\n",
      "|    std                  | 1.24           |\n",
      "|    value_loss           | 4.35e-05       |\n",
      "--------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 2308, ResetDay: 3988,Episode: 58\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 47           |\n",
      "|    time_elapsed         | 1286         |\n",
      "|    total_timesteps      | 96256        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 585.6854     |\n",
      "|    clip_fraction        | 0.769        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -45.9        |\n",
      "|    explained_variance   | -7.74        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.268       |\n",
      "|    n_updates            | 460          |\n",
      "|    policy_gradient_loss | 0.259        |\n",
      "|    reward               | 0.0001039854 |\n",
      "|    std                  | 1.25         |\n",
      "|    value_loss           | 0.000226     |\n",
      "------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 250, ResetDay: 1930,Episode: 59\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 74             |\n",
      "|    iterations           | 48             |\n",
      "|    time_elapsed         | 1313           |\n",
      "|    total_timesteps      | 98304          |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 47.12655       |\n",
      "|    clip_fraction        | 0.676          |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -46            |\n",
      "|    explained_variance   | -6.43          |\n",
      "|    learning_rate        | 0.00025        |\n",
      "|    loss                 | -0.321         |\n",
      "|    n_updates            | 470            |\n",
      "|    policy_gradient_loss | 0.142          |\n",
      "|    reward               | -0.00034999428 |\n",
      "|    std                  | 1.25           |\n",
      "|    value_loss           | 5.89e-05       |\n",
      "--------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 1174, ResetDay: 2854,Episode: 60\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 74             |\n",
      "|    iterations           | 49             |\n",
      "|    time_elapsed         | 1340           |\n",
      "|    total_timesteps      | 100352         |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 109.73598      |\n",
      "|    clip_fraction        | 0.713          |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -46.2          |\n",
      "|    explained_variance   | -6.08          |\n",
      "|    learning_rate        | 0.00025        |\n",
      "|    loss                 | -0.324         |\n",
      "|    n_updates            | 480            |\n",
      "|    policy_gradient_loss | 0.513          |\n",
      "|    reward               | -0.00023475417 |\n",
      "|    std                  | 1.26           |\n",
      "|    value_loss           | 0.000158       |\n",
      "--------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "day: 2854, episode: 60\n",
      "begin_total_asset: 500.00\n",
      "end_total_asset: 656.36\n",
      "total_reward: 156.36\n",
      "total_cost: 20.89\n",
      "total_trades: 46691\n",
      "Sharpe: 0.327\n",
      "=================================\n",
      "Reseting Environment StartDay: 671, ResetDay: 2351,Episode: 61\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 50            |\n",
      "|    time_elapsed         | 1368          |\n",
      "|    total_timesteps      | 102400        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 31.927048     |\n",
      "|    clip_fraction        | 0.804         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -46.3         |\n",
      "|    explained_variance   | -8.93         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.264        |\n",
      "|    n_updates            | 490           |\n",
      "|    policy_gradient_loss | 0.225         |\n",
      "|    reward               | -0.0005618385 |\n",
      "|    std                  | 1.27          |\n",
      "|    value_loss           | 0.000112      |\n",
      "-------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 881, ResetDay: 2561,Episode: 62\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 1395, ResetDay: 3075,Episode: 63\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 51            |\n",
      "|    time_elapsed         | 1395          |\n",
      "|    total_timesteps      | 104448        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 92.81739      |\n",
      "|    clip_fraction        | 0.712         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -46.4         |\n",
      "|    explained_variance   | -5.23         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.353        |\n",
      "|    n_updates            | 500           |\n",
      "|    policy_gradient_loss | 0.2           |\n",
      "|    reward               | 0.00048572914 |\n",
      "|    std                  | 1.27          |\n",
      "|    value_loss           | 0.0001        |\n",
      "-------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 1736, ResetDay: 3416,Episode: 64\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 52            |\n",
      "|    time_elapsed         | 1423          |\n",
      "|    total_timesteps      | 106496        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 83.66165      |\n",
      "|    clip_fraction        | 0.71          |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -46.5         |\n",
      "|    explained_variance   | -6.84         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.316        |\n",
      "|    n_updates            | 510           |\n",
      "|    policy_gradient_loss | 0.193         |\n",
      "|    reward               | 0.00018025437 |\n",
      "|    std                  | 1.27          |\n",
      "|    value_loss           | 7.16e-05      |\n",
      "-------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 2681, ResetDay: 4361,Episode: 65\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 53            |\n",
      "|    time_elapsed         | 1450          |\n",
      "|    total_timesteps      | 108544        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 40.310036     |\n",
      "|    clip_fraction        | 0.699         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -46.6         |\n",
      "|    explained_variance   | -6.59         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.375        |\n",
      "|    n_updates            | 520           |\n",
      "|    policy_gradient_loss | 0.191         |\n",
      "|    reward               | 0.00030212288 |\n",
      "|    std                  | 1.28          |\n",
      "|    value_loss           | 3.99e-05      |\n",
      "-------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "day: 4361, episode: 65\n",
      "begin_total_asset: 500.00\n",
      "end_total_asset: 505.35\n",
      "total_reward: 5.35\n",
      "total_cost: 22.93\n",
      "total_trades: 46914\n",
      "Sharpe: 0.172\n",
      "=================================\n",
      "Reseting Environment StartDay: 754, ResetDay: 2434,Episode: 66\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 54            |\n",
      "|    time_elapsed         | 1478          |\n",
      "|    total_timesteps      | 110592        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 91.47419      |\n",
      "|    clip_fraction        | 0.727         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -46.7         |\n",
      "|    explained_variance   | -5.65         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.35         |\n",
      "|    n_updates            | 530           |\n",
      "|    policy_gradient_loss | 0.188         |\n",
      "|    reward               | 0.00017504845 |\n",
      "|    std                  | 1.28          |\n",
      "|    value_loss           | 2.47e-05      |\n",
      "-------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 1877, ResetDay: 3557,Episode: 67\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 381, ResetDay: 2061,Episode: 68\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 55           |\n",
      "|    time_elapsed         | 1505         |\n",
      "|    total_timesteps      | 112640       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 298.92566    |\n",
      "|    clip_fraction        | 0.732        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -46.8        |\n",
      "|    explained_variance   | -4.75        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.38        |\n",
      "|    n_updates            | 540          |\n",
      "|    policy_gradient_loss | 0.152        |\n",
      "|    reward               | 0.0006913299 |\n",
      "|    std                  | 1.29         |\n",
      "|    value_loss           | 5.77e-05     |\n",
      "------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 2583, ResetDay: 4263,Episode: 69\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 74             |\n",
      "|    iterations           | 56             |\n",
      "|    time_elapsed         | 1532           |\n",
      "|    total_timesteps      | 114688         |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 73.570145      |\n",
      "|    clip_fraction        | 0.7            |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -46.9          |\n",
      "|    explained_variance   | -1.46          |\n",
      "|    learning_rate        | 0.00025        |\n",
      "|    loss                 | -0.353         |\n",
      "|    n_updates            | 550            |\n",
      "|    policy_gradient_loss | 0.156          |\n",
      "|    reward               | -0.00039429285 |\n",
      "|    std                  | 1.3            |\n",
      "|    value_loss           | 3.27e-05       |\n",
      "--------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 2488, ResetDay: 4168,Episode: 70\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 57            |\n",
      "|    time_elapsed         | 1560          |\n",
      "|    total_timesteps      | 116736        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 42.79557      |\n",
      "|    clip_fraction        | 0.766         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -47.1         |\n",
      "|    explained_variance   | -9.84         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.235        |\n",
      "|    n_updates            | 560           |\n",
      "|    policy_gradient_loss | 0.305         |\n",
      "|    reward               | 0.00053449784 |\n",
      "|    std                  | 1.3           |\n",
      "|    value_loss           | 9.51e-05      |\n",
      "-------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "day: 4168, episode: 70\n",
      "begin_total_asset: 500.00\n",
      "end_total_asset: 665.25\n",
      "total_reward: 165.25\n",
      "total_cost: 7.70\n",
      "total_trades: 46994\n",
      "Sharpe: 0.312\n",
      "=================================\n",
      "Reseting Environment StartDay: 2013, ResetDay: 3693,Episode: 71\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 74             |\n",
      "|    iterations           | 58             |\n",
      "|    time_elapsed         | 1587           |\n",
      "|    total_timesteps      | 118784         |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 143.66862      |\n",
      "|    clip_fraction        | 0.711          |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -47.2          |\n",
      "|    explained_variance   | -1.93          |\n",
      "|    learning_rate        | 0.00025        |\n",
      "|    loss                 | -0.301         |\n",
      "|    n_updates            | 570            |\n",
      "|    policy_gradient_loss | 0.205          |\n",
      "|    reward               | -0.00013234005 |\n",
      "|    std                  | 1.31           |\n",
      "|    value_loss           | 1.71e-05       |\n",
      "--------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 1561, ResetDay: 3241,Episode: 72\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 59            |\n",
      "|    time_elapsed         | 1615          |\n",
      "|    total_timesteps      | 120832        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 246.95688     |\n",
      "|    clip_fraction        | 0.711         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -47.3         |\n",
      "|    explained_variance   | -5.04         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.302        |\n",
      "|    n_updates            | 580           |\n",
      "|    policy_gradient_loss | 0.196         |\n",
      "|    reward               | 0.00015954551 |\n",
      "|    std                  | 1.32          |\n",
      "|    value_loss           | 1.88e-05      |\n",
      "-------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 21, ResetDay: 1701,Episode: 73\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 2088, ResetDay: 3768,Episode: 74\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 60           |\n",
      "|    time_elapsed         | 1642         |\n",
      "|    total_timesteps      | 122880       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 188.21454    |\n",
      "|    clip_fraction        | 0.73         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -47.5        |\n",
      "|    explained_variance   | -7.24        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.358       |\n",
      "|    n_updates            | 590          |\n",
      "|    policy_gradient_loss | 0.181        |\n",
      "|    reward               | 0.0006067478 |\n",
      "|    std                  | 1.32         |\n",
      "|    value_loss           | 1.89e-05     |\n",
      "------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 610, ResetDay: 2290,Episode: 75\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 61            |\n",
      "|    time_elapsed         | 1670          |\n",
      "|    total_timesteps      | 124928        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 154.83542     |\n",
      "|    clip_fraction        | 0.738         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -47.6         |\n",
      "|    explained_variance   | -8.2          |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.383        |\n",
      "|    n_updates            | 600           |\n",
      "|    policy_gradient_loss | 0.152         |\n",
      "|    reward               | 0.00030772274 |\n",
      "|    std                  | 1.33          |\n",
      "|    value_loss           | 0.000122      |\n",
      "-------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "day: 2290, episode: 75\n",
      "begin_total_asset: 500.00\n",
      "end_total_asset: 399.77\n",
      "total_reward: -100.23\n",
      "total_cost: 6.94\n",
      "total_trades: 47031\n",
      "Sharpe: 0.153\n",
      "=================================\n",
      "Reseting Environment StartDay: 821, ResetDay: 2501,Episode: 76\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 62           |\n",
      "|    time_elapsed         | 1697         |\n",
      "|    total_timesteps      | 126976       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 80.466125    |\n",
      "|    clip_fraction        | 0.739        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -47.7        |\n",
      "|    explained_variance   | -1.6         |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.348       |\n",
      "|    n_updates            | 610          |\n",
      "|    policy_gradient_loss | 0.179        |\n",
      "|    reward               | 0.0003007286 |\n",
      "|    std                  | 1.33         |\n",
      "|    value_loss           | 3.61e-05     |\n",
      "------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 2326, ResetDay: 4006,Episode: 77\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 63            |\n",
      "|    time_elapsed         | 1724          |\n",
      "|    total_timesteps      | 129024        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 39.52388      |\n",
      "|    clip_fraction        | 0.7           |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -47.8         |\n",
      "|    explained_variance   | -8.13         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.33         |\n",
      "|    n_updates            | 620           |\n",
      "|    policy_gradient_loss | 0.171         |\n",
      "|    reward               | 0.00039988672 |\n",
      "|    std                  | 1.34          |\n",
      "|    value_loss           | 3.82e-05      |\n",
      "-------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 1117, ResetDay: 2797,Episode: 78\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 64            |\n",
      "|    time_elapsed         | 1752          |\n",
      "|    total_timesteps      | 131072        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 49.90557      |\n",
      "|    clip_fraction        | 0.706         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -48           |\n",
      "|    explained_variance   | -3.68         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.309        |\n",
      "|    n_updates            | 630           |\n",
      "|    policy_gradient_loss | 0.16          |\n",
      "|    reward               | 0.00031599807 |\n",
      "|    std                  | 1.35          |\n",
      "|    value_loss           | 1.51e-05      |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 530, ResetDay: 2210,Episode: 79\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 15, ResetDay: 1695,Episode: 80\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 74             |\n",
      "|    iterations           | 65             |\n",
      "|    time_elapsed         | 1779           |\n",
      "|    total_timesteps      | 133120         |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 273.43304      |\n",
      "|    clip_fraction        | 0.71           |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -48.1          |\n",
      "|    explained_variance   | -4.73          |\n",
      "|    learning_rate        | 0.00025        |\n",
      "|    loss                 | -0.317         |\n",
      "|    n_updates            | 640            |\n",
      "|    policy_gradient_loss | 0.156          |\n",
      "|    reward               | -0.00021998654 |\n",
      "|    std                  | 1.35           |\n",
      "|    value_loss           | 1.99e-05       |\n",
      "--------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "day: 1695, episode: 80\n",
      "begin_total_asset: 500.00\n",
      "end_total_asset: 344.33\n",
      "total_reward: -155.67\n",
      "total_cost: 5.41\n",
      "total_trades: 47032\n",
      "Sharpe: -0.053\n",
      "=================================\n",
      "Reseting Environment StartDay: 1977, ResetDay: 3657,Episode: 81\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 74             |\n",
      "|    iterations           | 66             |\n",
      "|    time_elapsed         | 1807           |\n",
      "|    total_timesteps      | 135168         |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 101.58447      |\n",
      "|    clip_fraction        | 0.698          |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -48.3          |\n",
      "|    explained_variance   | -6.02          |\n",
      "|    learning_rate        | 0.00025        |\n",
      "|    loss                 | -0.399         |\n",
      "|    n_updates            | 650            |\n",
      "|    policy_gradient_loss | 0.143          |\n",
      "|    reward               | -0.00046587066 |\n",
      "|    std                  | 1.36           |\n",
      "|    value_loss           | 5.73e-05       |\n",
      "--------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 1011, ResetDay: 2691,Episode: 82\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 67            |\n",
      "|    time_elapsed         | 1835          |\n",
      "|    total_timesteps      | 137216        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 31.63956      |\n",
      "|    clip_fraction        | 0.709         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -48.4         |\n",
      "|    explained_variance   | -4.8          |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.296        |\n",
      "|    n_updates            | 660           |\n",
      "|    policy_gradient_loss | 0.164         |\n",
      "|    reward               | 3.2432556e-05 |\n",
      "|    std                  | 1.37          |\n",
      "|    value_loss           | 3.86e-05      |\n",
      "-------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 30, ResetDay: 1710,Episode: 83\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 68           |\n",
      "|    time_elapsed         | 1862         |\n",
      "|    total_timesteps      | 139264       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 102.025665   |\n",
      "|    clip_fraction        | 0.702        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -48.5        |\n",
      "|    explained_variance   | -2.09        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.286       |\n",
      "|    n_updates            | 670          |\n",
      "|    policy_gradient_loss | 0.166        |\n",
      "|    reward               | 0.0006570693 |\n",
      "|    std                  | 1.38         |\n",
      "|    value_loss           | 1.65e-05     |\n",
      "------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 2093, ResetDay: 3773,Episode: 84\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 1537, ResetDay: 3217,Episode: 85\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 74             |\n",
      "|    iterations           | 69             |\n",
      "|    time_elapsed         | 1890           |\n",
      "|    total_timesteps      | 141312         |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 64.5721        |\n",
      "|    clip_fraction        | 0.739          |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -48.7          |\n",
      "|    explained_variance   | -8.02          |\n",
      "|    learning_rate        | 0.00025        |\n",
      "|    loss                 | -0.357         |\n",
      "|    n_updates            | 680            |\n",
      "|    policy_gradient_loss | 0.179          |\n",
      "|    reward               | -2.6448346e-05 |\n",
      "|    std                  | 1.38           |\n",
      "|    value_loss           | 4.35e-05       |\n",
      "--------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "day: 3217, episode: 85\n",
      "begin_total_asset: 500.00\n",
      "end_total_asset: 460.61\n",
      "total_reward: -39.39\n",
      "total_cost: 4.94\n",
      "total_trades: 47029\n",
      "Sharpe: 0.150\n",
      "=================================\n",
      "Reseting Environment StartDay: 2392, ResetDay: 4072,Episode: 86\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 70           |\n",
      "|    time_elapsed         | 1917         |\n",
      "|    total_timesteps      | 143360       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 45.26597     |\n",
      "|    clip_fraction        | 0.721        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -48.8        |\n",
      "|    explained_variance   | -0.697       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.351       |\n",
      "|    n_updates            | 690          |\n",
      "|    policy_gradient_loss | 0.284        |\n",
      "|    reward               | -0.000654792 |\n",
      "|    std                  | 1.39         |\n",
      "|    value_loss           | 1.49e-05     |\n",
      "------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 698, ResetDay: 2378,Episode: 87\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 71            |\n",
      "|    time_elapsed         | 1945          |\n",
      "|    total_timesteps      | 145408        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 40.395416     |\n",
      "|    clip_fraction        | 0.714         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -48.9         |\n",
      "|    explained_variance   | -3.34         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.34         |\n",
      "|    n_updates            | 700           |\n",
      "|    policy_gradient_loss | 0.192         |\n",
      "|    reward               | 0.00024502183 |\n",
      "|    std                  | 1.39          |\n",
      "|    value_loss           | 1.54e-05      |\n",
      "-------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 310, ResetDay: 1990,Episode: 88\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 72            |\n",
      "|    time_elapsed         | 1972          |\n",
      "|    total_timesteps      | 147456        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 108.45577     |\n",
      "|    clip_fraction        | 0.708         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -49           |\n",
      "|    explained_variance   | -3.2          |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.328        |\n",
      "|    n_updates            | 710           |\n",
      "|    policy_gradient_loss | 0.136         |\n",
      "|    reward               | 0.00021091232 |\n",
      "|    std                  | 1.4           |\n",
      "|    value_loss           | 1.94e-05      |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 2011, ResetDay: 3691,Episode: 89\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 73            |\n",
      "|    time_elapsed         | 2000          |\n",
      "|    total_timesteps      | 149504        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 42.552864     |\n",
      "|    clip_fraction        | 0.709         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -49.2         |\n",
      "|    explained_variance   | -5.33         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.422        |\n",
      "|    n_updates            | 720           |\n",
      "|    policy_gradient_loss | 0.15          |\n",
      "|    reward               | 0.00060137484 |\n",
      "|    std                  | 1.41          |\n",
      "|    value_loss           | 2.29e-05      |\n",
      "-------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 1860, ResetDay: 3540,Episode: 90\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "day: 3540, episode: 90\n",
      "begin_total_asset: 500.00\n",
      "end_total_asset: 442.56\n",
      "total_reward: -57.44\n",
      "total_cost: 4.92\n",
      "total_trades: 47040\n",
      "Sharpe: 0.100\n",
      "=================================\n",
      "Reseting Environment StartDay: 2022, ResetDay: 3702,Episode: 91\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 74            |\n",
      "|    time_elapsed         | 2027          |\n",
      "|    total_timesteps      | 151552        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 43.725227     |\n",
      "|    clip_fraction        | 0.74          |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -49.3         |\n",
      "|    explained_variance   | -1.37         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.361        |\n",
      "|    n_updates            | 730           |\n",
      "|    policy_gradient_loss | 0.227         |\n",
      "|    reward               | -3.717041e-05 |\n",
      "|    std                  | 1.41          |\n",
      "|    value_loss           | 8.95e-06      |\n",
      "-------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 96, ResetDay: 1776,Episode: 92\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 75           |\n",
      "|    time_elapsed         | 2055         |\n",
      "|    total_timesteps      | 153600       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 257.92166    |\n",
      "|    clip_fraction        | 0.712        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -49.4        |\n",
      "|    explained_variance   | -1.74        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.327       |\n",
      "|    n_updates            | 740          |\n",
      "|    policy_gradient_loss | 0.268        |\n",
      "|    reward               | 6.379843e-05 |\n",
      "|    std                  | 1.41         |\n",
      "|    value_loss           | 5.63e-06     |\n",
      "------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 1508, ResetDay: 3188,Episode: 93\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 76            |\n",
      "|    time_elapsed         | 2083          |\n",
      "|    total_timesteps      | 155648        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 73.238625     |\n",
      "|    clip_fraction        | 0.79          |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -49.4         |\n",
      "|    explained_variance   | -3.57         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.363        |\n",
      "|    n_updates            | 750           |\n",
      "|    policy_gradient_loss | 0.235         |\n",
      "|    reward               | -0.0002293167 |\n",
      "|    std                  | 1.42          |\n",
      "|    value_loss           | 1.86e-05      |\n",
      "-------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 291, ResetDay: 1971,Episode: 94\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 77            |\n",
      "|    time_elapsed         | 2111          |\n",
      "|    total_timesteps      | 157696        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 33.471687     |\n",
      "|    clip_fraction        | 0.687         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -49.6         |\n",
      "|    explained_variance   | -3            |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.387        |\n",
      "|    n_updates            | 760           |\n",
      "|    policy_gradient_loss | 0.166         |\n",
      "|    reward               | 0.00020002766 |\n",
      "|    std                  | 1.43          |\n",
      "|    value_loss           | 1.52e-05      |\n",
      "-------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 189, ResetDay: 1869,Episode: 95\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "day: 1869, episode: 95\n",
      "begin_total_asset: 500.00\n",
      "end_total_asset: 501.75\n",
      "total_reward: 1.75\n",
      "total_cost: 4.95\n",
      "total_trades: 47040\n",
      "Sharpe: 0.117\n",
      "=================================\n",
      "Reseting Environment StartDay: 2724, ResetDay: 4404,Episode: 96\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 78            |\n",
      "|    time_elapsed         | 2139          |\n",
      "|    total_timesteps      | 159744        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 90.26879      |\n",
      "|    clip_fraction        | 0.751         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -49.7         |\n",
      "|    explained_variance   | -2.83         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.444        |\n",
      "|    n_updates            | 770           |\n",
      "|    policy_gradient_loss | 0.203         |\n",
      "|    reward               | -0.0014952725 |\n",
      "|    std                  | 1.44          |\n",
      "|    value_loss           | 1.76e-05      |\n",
      "-------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 331, ResetDay: 2011,Episode: 97\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 74          |\n",
      "|    iterations           | 79          |\n",
      "|    time_elapsed         | 2166        |\n",
      "|    total_timesteps      | 161792      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 38.491314   |\n",
      "|    clip_fraction        | 0.754       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -50         |\n",
      "|    explained_variance   | -3.69       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.415      |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | 0.203       |\n",
      "|    reward               | 0.000609704 |\n",
      "|    std                  | 1.45        |\n",
      "|    value_loss           | 2.03e-05    |\n",
      "-----------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 2292, ResetDay: 3972,Episode: 98\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 80            |\n",
      "|    time_elapsed         | 2194          |\n",
      "|    total_timesteps      | 163840        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 94.22412      |\n",
      "|    clip_fraction        | 0.724         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -50.1         |\n",
      "|    explained_variance   | -0.289        |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.357        |\n",
      "|    n_updates            | 790           |\n",
      "|    policy_gradient_loss | 0.206         |\n",
      "|    reward               | 0.00077145995 |\n",
      "|    std                  | 1.46          |\n",
      "|    value_loss           | 1.1e-05       |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 1987, ResetDay: 3667,Episode: 99\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 81            |\n",
      "|    time_elapsed         | 2221          |\n",
      "|    total_timesteps      | 165888        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 35.15053      |\n",
      "|    clip_fraction        | 0.73          |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -50.2         |\n",
      "|    explained_variance   | -3.16         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.428        |\n",
      "|    n_updates            | 800           |\n",
      "|    policy_gradient_loss | 0.152         |\n",
      "|    reward               | 0.00025190468 |\n",
      "|    std                  | 1.46          |\n",
      "|    value_loss           | 1.01e-05      |\n",
      "-------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 1596, ResetDay: 3276,Episode: 100\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 82            |\n",
      "|    time_elapsed         | 2249          |\n",
      "|    total_timesteps      | 167936        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 139.95345     |\n",
      "|    clip_fraction        | 0.723         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -50.3         |\n",
      "|    explained_variance   | -0.49         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.162        |\n",
      "|    n_updates            | 810           |\n",
      "|    policy_gradient_loss | 0.225         |\n",
      "|    reward               | -5.468521e-05 |\n",
      "|    std                  | 1.46          |\n",
      "|    value_loss           | 6.35e-06      |\n",
      "-------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "day: 3276, episode: 100\n",
      "begin_total_asset: 500.00\n",
      "end_total_asset: 354.67\n",
      "total_reward: -145.33\n",
      "total_cost: 4.94\n",
      "total_trades: 47030\n",
      "Sharpe: 0.084\n",
      "=================================\n",
      "Reseting Environment StartDay: 2573, ResetDay: 4253,Episode: 101\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 2789, ResetDay: 4469,Episode: 102\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 83           |\n",
      "|    time_elapsed         | 2276         |\n",
      "|    total_timesteps      | 169984       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 159.85895    |\n",
      "|    clip_fraction        | 0.746        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -50.4        |\n",
      "|    explained_variance   | -1.54        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.388       |\n",
      "|    n_updates            | 820          |\n",
      "|    policy_gradient_loss | 0.261        |\n",
      "|    reward               | 0.0001675171 |\n",
      "|    std                  | 1.47         |\n",
      "|    value_loss           | 5.85e-06     |\n",
      "------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 1196, ResetDay: 2876,Episode: 103\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 74             |\n",
      "|    iterations           | 84             |\n",
      "|    time_elapsed         | 2303           |\n",
      "|    total_timesteps      | 172032         |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 148.52928      |\n",
      "|    clip_fraction        | 0.725          |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -50.5          |\n",
      "|    explained_variance   | -0.165         |\n",
      "|    learning_rate        | 0.00025        |\n",
      "|    loss                 | -0.227         |\n",
      "|    n_updates            | 830            |\n",
      "|    policy_gradient_loss | 0.206          |\n",
      "|    reward               | -0.00016028271 |\n",
      "|    std                  | 1.48           |\n",
      "|    value_loss           | 4.35e-06       |\n",
      "--------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 1014, ResetDay: 2694,Episode: 104\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 85            |\n",
      "|    time_elapsed         | 2331          |\n",
      "|    total_timesteps      | 174080        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 129.01895     |\n",
      "|    clip_fraction        | 0.706         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -50.7         |\n",
      "|    explained_variance   | -1.05         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.437        |\n",
      "|    n_updates            | 840           |\n",
      "|    policy_gradient_loss | 0.156         |\n",
      "|    reward               | 0.00040301742 |\n",
      "|    std                  | 1.48          |\n",
      "|    value_loss           | 5.84e-06      |\n",
      "-------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 246, ResetDay: 1926,Episode: 105\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 74             |\n",
      "|    iterations           | 86             |\n",
      "|    time_elapsed         | 2358           |\n",
      "|    total_timesteps      | 176128         |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 70.85966       |\n",
      "|    clip_fraction        | 0.734          |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -50.8          |\n",
      "|    explained_variance   | -3.51          |\n",
      "|    learning_rate        | 0.00025        |\n",
      "|    loss                 | 0.234          |\n",
      "|    n_updates            | 850            |\n",
      "|    policy_gradient_loss | 0.257          |\n",
      "|    reward               | -6.8279645e-05 |\n",
      "|    std                  | 1.49           |\n",
      "|    value_loss           | 6.03e-06       |\n",
      "--------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "day: 1926, episode: 105\n",
      "begin_total_asset: 500.00\n",
      "end_total_asset: 395.16\n",
      "total_reward: -104.84\n",
      "total_cost: 6.15\n",
      "total_trades: 47036\n",
      "Sharpe: 0.013\n",
      "=================================\n",
      "Reseting Environment StartDay: 10, ResetDay: 1690,Episode: 106\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 74             |\n",
      "|    iterations           | 87             |\n",
      "|    time_elapsed         | 2386           |\n",
      "|    total_timesteps      | 178176         |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 48.540554      |\n",
      "|    clip_fraction        | 0.749          |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -50.9          |\n",
      "|    explained_variance   | -3.69          |\n",
      "|    learning_rate        | 0.00025        |\n",
      "|    loss                 | -0.369         |\n",
      "|    n_updates            | 860            |\n",
      "|    policy_gradient_loss | 0.174          |\n",
      "|    reward               | -5.5132674e-05 |\n",
      "|    std                  | 1.5            |\n",
      "|    value_loss           | 1.25e-05       |\n",
      "--------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 956, ResetDay: 2636,Episode: 107\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 2726, ResetDay: 4406,Episode: 108\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 88            |\n",
      "|    time_elapsed         | 2413          |\n",
      "|    total_timesteps      | 180224        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 41.08314      |\n",
      "|    clip_fraction        | 0.731         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -51.1         |\n",
      "|    explained_variance   | -3.94         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.411        |\n",
      "|    n_updates            | 870           |\n",
      "|    policy_gradient_loss | 0.163         |\n",
      "|    reward               | 5.1352312e-05 |\n",
      "|    std                  | 1.5           |\n",
      "|    value_loss           | 1.4e-05       |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 2048, ResetDay: 3728,Episode: 109\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 89            |\n",
      "|    time_elapsed         | 2441          |\n",
      "|    total_timesteps      | 182272        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 33.609833     |\n",
      "|    clip_fraction        | 0.687         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -51.2         |\n",
      "|    explained_variance   | -0.95         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.263        |\n",
      "|    n_updates            | 880           |\n",
      "|    policy_gradient_loss | 0.156         |\n",
      "|    reward               | 0.00020980988 |\n",
      "|    std                  | 1.51          |\n",
      "|    value_loss           | 6.52e-06      |\n",
      "-------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 635, ResetDay: 2315,Episode: 110\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 90            |\n",
      "|    time_elapsed         | 2468          |\n",
      "|    total_timesteps      | 184320        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 137.30884     |\n",
      "|    clip_fraction        | 0.703         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -51.3         |\n",
      "|    explained_variance   | -0.0276       |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.38         |\n",
      "|    n_updates            | 890           |\n",
      "|    policy_gradient_loss | 0.204         |\n",
      "|    reward               | -0.0001040905 |\n",
      "|    std                  | 1.52          |\n",
      "|    value_loss           | 5.59e-06      |\n",
      "-------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "day: 2315, episode: 110\n",
      "begin_total_asset: 500.00\n",
      "end_total_asset: 329.31\n",
      "total_reward: -170.69\n",
      "total_cost: 5.23\n",
      "total_trades: 47031\n",
      "Sharpe: 0.069\n",
      "=================================\n",
      "Reseting Environment StartDay: 960, ResetDay: 2640,Episode: 111\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 91           |\n",
      "|    time_elapsed         | 2496         |\n",
      "|    total_timesteps      | 186368       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 88.06683     |\n",
      "|    clip_fraction        | 0.789        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -51.4        |\n",
      "|    explained_variance   | -1.77        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.295       |\n",
      "|    n_updates            | 900          |\n",
      "|    policy_gradient_loss | 0.181        |\n",
      "|    reward               | 0.0002022953 |\n",
      "|    std                  | 1.53         |\n",
      "|    value_loss           | 7.47e-06     |\n",
      "------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 2072, ResetDay: 3752,Episode: 112\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 1703, ResetDay: 3383,Episode: 113\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 74             |\n",
      "|    iterations           | 92             |\n",
      "|    time_elapsed         | 2523           |\n",
      "|    total_timesteps      | 188416         |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 42.88789       |\n",
      "|    clip_fraction        | 0.731          |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -51.6          |\n",
      "|    explained_variance   | -1.71          |\n",
      "|    learning_rate        | 0.00025        |\n",
      "|    loss                 | -0.362         |\n",
      "|    n_updates            | 910            |\n",
      "|    policy_gradient_loss | 0.183          |\n",
      "|    reward               | -0.00016105271 |\n",
      "|    std                  | 1.53           |\n",
      "|    value_loss           | 4.62e-06       |\n",
      "--------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 1401, ResetDay: 3081,Episode: 114\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 74             |\n",
      "|    iterations           | 93             |\n",
      "|    time_elapsed         | 2551           |\n",
      "|    total_timesteps      | 190464         |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 76.81086       |\n",
      "|    clip_fraction        | 0.705          |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -51.7          |\n",
      "|    explained_variance   | -0.166         |\n",
      "|    learning_rate        | 0.00025        |\n",
      "|    loss                 | -0.285         |\n",
      "|    n_updates            | 920            |\n",
      "|    policy_gradient_loss | 0.177          |\n",
      "|    reward               | -0.00028241778 |\n",
      "|    std                  | 1.54           |\n",
      "|    value_loss           | 4.56e-06       |\n",
      "--------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 2035, ResetDay: 3715,Episode: 115\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 94            |\n",
      "|    time_elapsed         | 2578          |\n",
      "|    total_timesteps      | 192512        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 40.79418      |\n",
      "|    clip_fraction        | 0.721         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -51.8         |\n",
      "|    explained_variance   | -0.672        |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.362        |\n",
      "|    n_updates            | 930           |\n",
      "|    policy_gradient_loss | 0.175         |\n",
      "|    reward               | -4.094162e-05 |\n",
      "|    std                  | 1.55          |\n",
      "|    value_loss           | 4.93e-06      |\n",
      "-------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "day: 3715, episode: 115\n",
      "begin_total_asset: 500.00\n",
      "end_total_asset: 719.02\n",
      "total_reward: 219.02\n",
      "total_cost: 4.84\n",
      "total_trades: 47030\n",
      "Sharpe: 0.407\n",
      "=================================\n",
      "Reseting Environment StartDay: 2, ResetDay: 1682,Episode: 116\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 95            |\n",
      "|    time_elapsed         | 2606          |\n",
      "|    total_timesteps      | 194560        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 41.25508      |\n",
      "|    clip_fraction        | 0.688         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -52           |\n",
      "|    explained_variance   | -0.509        |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.187        |\n",
      "|    n_updates            | 940           |\n",
      "|    policy_gradient_loss | 0.179         |\n",
      "|    reward               | 0.00017730966 |\n",
      "|    std                  | 1.55          |\n",
      "|    value_loss           | 2.47e-06      |\n",
      "-------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 1786, ResetDay: 3466,Episode: 117\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 96            |\n",
      "|    time_elapsed         | 2633          |\n",
      "|    total_timesteps      | 196608        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 88.02344      |\n",
      "|    clip_fraction        | 0.784         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -52.1         |\n",
      "|    explained_variance   | -2.7          |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.331        |\n",
      "|    n_updates            | 950           |\n",
      "|    policy_gradient_loss | 0.248         |\n",
      "|    reward               | -0.0008756321 |\n",
      "|    std                  | 1.56          |\n",
      "|    value_loss           | 1.66e-05      |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 2360, ResetDay: 4040,Episode: 118\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 579, ResetDay: 2259,Episode: 119\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 97           |\n",
      "|    time_elapsed         | 2661         |\n",
      "|    total_timesteps      | 198656       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 34.853977    |\n",
      "|    clip_fraction        | 0.738        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -52.2        |\n",
      "|    explained_variance   | -0.26        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.388       |\n",
      "|    n_updates            | 960          |\n",
      "|    policy_gradient_loss | 0.302        |\n",
      "|    reward               | 0.0001691637 |\n",
      "|    std                  | 1.57         |\n",
      "|    value_loss           | 6.57e-06     |\n",
      "------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 1151, ResetDay: 2831,Episode: 120\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 98            |\n",
      "|    time_elapsed         | 2688          |\n",
      "|    total_timesteps      | 200704        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 190.5682      |\n",
      "|    clip_fraction        | 0.779         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -52.4         |\n",
      "|    explained_variance   | -0.298        |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.404        |\n",
      "|    n_updates            | 970           |\n",
      "|    policy_gradient_loss | 0.466         |\n",
      "|    reward               | 0.00011498995 |\n",
      "|    std                  | 1.58          |\n",
      "|    value_loss           | 7.26e-06      |\n",
      "-------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "day: 2831, episode: 120\n",
      "begin_total_asset: 500.00\n",
      "end_total_asset: 567.37\n",
      "total_reward: 67.37\n",
      "total_cost: 4.94\n",
      "total_trades: 47030\n",
      "Sharpe: 0.220\n",
      "=================================\n",
      "Reseting Environment StartDay: 531, ResetDay: 2211,Episode: 121\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 74             |\n",
      "|    iterations           | 99             |\n",
      "|    time_elapsed         | 2716           |\n",
      "|    total_timesteps      | 202752         |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 24.055416      |\n",
      "|    clip_fraction        | 0.756          |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -52.5          |\n",
      "|    explained_variance   | -3.29          |\n",
      "|    learning_rate        | 0.00025        |\n",
      "|    loss                 | -0.4           |\n",
      "|    n_updates            | 980            |\n",
      "|    policy_gradient_loss | 0.21           |\n",
      "|    reward               | -0.00036973372 |\n",
      "|    std                  | 1.58           |\n",
      "|    value_loss           | 5.38e-06       |\n",
      "--------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 2787, ResetDay: 4467,Episode: 122\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 74             |\n",
      "|    iterations           | 100            |\n",
      "|    time_elapsed         | 2743           |\n",
      "|    total_timesteps      | 204800         |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 38.141575      |\n",
      "|    clip_fraction        | 0.766          |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -52.5          |\n",
      "|    explained_variance   | -1.82          |\n",
      "|    learning_rate        | 0.00025        |\n",
      "|    loss                 | -0.367         |\n",
      "|    n_updates            | 990            |\n",
      "|    policy_gradient_loss | 0.229          |\n",
      "|    reward               | -0.00042072716 |\n",
      "|    std                  | 1.58           |\n",
      "|    value_loss           | 6.54e-06       |\n",
      "--------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 3, ResetDay: 1683,Episode: 123\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 2476, ResetDay: 4156,Episode: 124\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 74             |\n",
      "|    iterations           | 101            |\n",
      "|    time_elapsed         | 2771           |\n",
      "|    total_timesteps      | 206848         |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 36.277687      |\n",
      "|    clip_fraction        | 0.762          |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -52.6          |\n",
      "|    explained_variance   | -0.0395        |\n",
      "|    learning_rate        | 0.00025        |\n",
      "|    loss                 | -0.128         |\n",
      "|    n_updates            | 1000           |\n",
      "|    policy_gradient_loss | 0.269          |\n",
      "|    reward               | -0.00034564076 |\n",
      "|    std                  | 1.59           |\n",
      "|    value_loss           | 3.56e-06       |\n",
      "--------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 1351, ResetDay: 3031,Episode: 125\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 102           |\n",
      "|    time_elapsed         | 2798          |\n",
      "|    total_timesteps      | 208896        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 405.01785     |\n",
      "|    clip_fraction        | 0.734         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -52.6         |\n",
      "|    explained_variance   | -1.39         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.423        |\n",
      "|    n_updates            | 1010          |\n",
      "|    policy_gradient_loss | 0.17          |\n",
      "|    reward               | 0.00015355034 |\n",
      "|    std                  | 1.59          |\n",
      "|    value_loss           | 1.69e-05      |\n",
      "-------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "day: 3031, episode: 125\n",
      "begin_total_asset: 500.00\n",
      "end_total_asset: 366.02\n",
      "total_reward: -133.98\n",
      "total_cost: 5.23\n",
      "total_trades: 47030\n",
      "Sharpe: 0.105\n",
      "=================================\n",
      "Reseting Environment StartDay: 2674, ResetDay: 4354,Episode: 126\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 74          |\n",
      "|    iterations           | 103         |\n",
      "|    time_elapsed         | 2826        |\n",
      "|    total_timesteps      | 210944      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 78.47195    |\n",
      "|    clip_fraction        | 0.71        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -52.7       |\n",
      "|    explained_variance   | -0.0286     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.171      |\n",
      "|    n_updates            | 1020        |\n",
      "|    policy_gradient_loss | 0.202       |\n",
      "|    reward               | 0.001001958 |\n",
      "|    std                  | 1.6         |\n",
      "|    value_loss           | 1.02e-05    |\n",
      "-----------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 2093, ResetDay: 3773,Episode: 127\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 74             |\n",
      "|    iterations           | 104            |\n",
      "|    time_elapsed         | 2853           |\n",
      "|    total_timesteps      | 212992         |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 36.71846       |\n",
      "|    clip_fraction        | 0.742          |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -52.9          |\n",
      "|    explained_variance   | -0.409         |\n",
      "|    learning_rate        | 0.00025        |\n",
      "|    loss                 | -0.392         |\n",
      "|    n_updates            | 1030           |\n",
      "|    policy_gradient_loss | 0.366          |\n",
      "|    reward               | -0.00011695137 |\n",
      "|    std                  | 1.6            |\n",
      "|    value_loss           | 2.33e-06       |\n",
      "--------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 499, ResetDay: 2179,Episode: 128\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 74             |\n",
      "|    iterations           | 105            |\n",
      "|    time_elapsed         | 2881           |\n",
      "|    total_timesteps      | 215040         |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 200.30421      |\n",
      "|    clip_fraction        | 0.674          |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -53            |\n",
      "|    explained_variance   | 0.00879        |\n",
      "|    learning_rate        | 0.00025        |\n",
      "|    loss                 | -0.483         |\n",
      "|    n_updates            | 1040           |\n",
      "|    policy_gradient_loss | 0.143          |\n",
      "|    reward               | -1.1013221e-05 |\n",
      "|    std                  | 1.61           |\n",
      "|    value_loss           | 4.26e-06       |\n",
      "--------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 2708, ResetDay: 4388,Episode: 129\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 2323, ResetDay: 4003,Episode: 130\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 106           |\n",
      "|    time_elapsed         | 2908          |\n",
      "|    total_timesteps      | 217088        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 165.23589     |\n",
      "|    clip_fraction        | 0.752         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -53.1         |\n",
      "|    explained_variance   | -1.72         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.403        |\n",
      "|    n_updates            | 1050          |\n",
      "|    policy_gradient_loss | 0.199         |\n",
      "|    reward               | -0.0005298462 |\n",
      "|    std                  | 1.62          |\n",
      "|    value_loss           | 5.23e-06      |\n",
      "-------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "day: 4003, episode: 130\n",
      "begin_total_asset: 500.00\n",
      "end_total_asset: 529.49\n",
      "total_reward: 29.49\n",
      "total_cost: 4.94\n",
      "total_trades: 47030\n",
      "Sharpe: 0.166\n",
      "=================================\n",
      "Reseting Environment StartDay: 1504, ResetDay: 3184,Episode: 131\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 74             |\n",
      "|    iterations           | 107            |\n",
      "|    time_elapsed         | 2936           |\n",
      "|    total_timesteps      | 219136         |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 52.844635      |\n",
      "|    clip_fraction        | 0.698          |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -53.3          |\n",
      "|    explained_variance   | 0.0344         |\n",
      "|    learning_rate        | 0.00025        |\n",
      "|    loss                 | -0.387         |\n",
      "|    n_updates            | 1060           |\n",
      "|    policy_gradient_loss | 0.157          |\n",
      "|    reward               | -0.00016148377 |\n",
      "|    std                  | 1.63           |\n",
      "|    value_loss           | 5.04e-06       |\n",
      "--------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 2663, ResetDay: 4343,Episode: 132\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 108           |\n",
      "|    time_elapsed         | 2964          |\n",
      "|    total_timesteps      | 221184        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 76.44656      |\n",
      "|    clip_fraction        | 0.719         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -53.5         |\n",
      "|    explained_variance   | -0.191        |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.371        |\n",
      "|    n_updates            | 1070          |\n",
      "|    policy_gradient_loss | 0.191         |\n",
      "|    reward               | 0.00013282089 |\n",
      "|    std                  | 1.64          |\n",
      "|    value_loss           | 4.41e-06      |\n",
      "-------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 2179, ResetDay: 3859,Episode: 133\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 109           |\n",
      "|    time_elapsed         | 2991          |\n",
      "|    total_timesteps      | 223232        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 51.09292      |\n",
      "|    clip_fraction        | 0.716         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -53.7         |\n",
      "|    explained_variance   | -0.114        |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.293        |\n",
      "|    n_updates            | 1080          |\n",
      "|    policy_gradient_loss | 0.179         |\n",
      "|    reward               | 5.6359862e-05 |\n",
      "|    std                  | 1.65          |\n",
      "|    value_loss           | 4.43e-06      |\n",
      "-------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 331, ResetDay: 2011,Episode: 134\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 230, ResetDay: 1910,Episode: 135\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 74          |\n",
      "|    iterations           | 110         |\n",
      "|    time_elapsed         | 3018        |\n",
      "|    total_timesteps      | 225280      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 194.52573   |\n",
      "|    clip_fraction        | 0.75        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -53.8       |\n",
      "|    explained_variance   | 0.0251      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.33       |\n",
      "|    n_updates            | 1090        |\n",
      "|    policy_gradient_loss | 0.182       |\n",
      "|    reward               | 3.82015e-05 |\n",
      "|    std                  | 1.66        |\n",
      "|    value_loss           | 7.34e-06    |\n",
      "-----------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "day: 1910, episode: 135\n",
      "begin_total_asset: 500.00\n",
      "end_total_asset: 447.77\n",
      "total_reward: -52.23\n",
      "total_cost: 5.19\n",
      "total_trades: 47036\n",
      "Sharpe: 0.075\n",
      "=================================\n",
      "Reseting Environment StartDay: 997, ResetDay: 2677,Episode: 136\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 74             |\n",
      "|    iterations           | 111            |\n",
      "|    time_elapsed         | 3046           |\n",
      "|    total_timesteps      | 227328         |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 185.83119      |\n",
      "|    clip_fraction        | 0.722          |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -53.9          |\n",
      "|    explained_variance   | -1.74          |\n",
      "|    learning_rate        | 0.00025        |\n",
      "|    loss                 | -0.345         |\n",
      "|    n_updates            | 1100           |\n",
      "|    policy_gradient_loss | 0.181          |\n",
      "|    reward               | -0.00015310745 |\n",
      "|    std                  | 1.67           |\n",
      "|    value_loss           | 5.91e-06       |\n",
      "--------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 397, ResetDay: 2077,Episode: 137\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 74             |\n",
      "|    iterations           | 112            |\n",
      "|    time_elapsed         | 3073           |\n",
      "|    total_timesteps      | 229376         |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 28.485874      |\n",
      "|    clip_fraction        | 0.766          |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -54            |\n",
      "|    explained_variance   | -2.21          |\n",
      "|    learning_rate        | 0.00025        |\n",
      "|    loss                 | -0.34          |\n",
      "|    n_updates            | 1110           |\n",
      "|    policy_gradient_loss | 0.214          |\n",
      "|    reward               | -0.00018023768 |\n",
      "|    std                  | 1.67           |\n",
      "|    value_loss           | 5.02e-06       |\n",
      "--------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 876, ResetDay: 2556,Episode: 138\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 74             |\n",
      "|    iterations           | 113            |\n",
      "|    time_elapsed         | 3101           |\n",
      "|    total_timesteps      | 231424         |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 33.73643       |\n",
      "|    clip_fraction        | 0.732          |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -54.1          |\n",
      "|    explained_variance   | -0.592         |\n",
      "|    learning_rate        | 0.00025        |\n",
      "|    loss                 | -0.422         |\n",
      "|    n_updates            | 1120           |\n",
      "|    policy_gradient_loss | 0.245          |\n",
      "|    reward               | -0.00013896008 |\n",
      "|    std                  | 1.68           |\n",
      "|    value_loss           | 5.39e-06       |\n",
      "--------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 1565, ResetDay: 3245,Episode: 139\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 114          |\n",
      "|    time_elapsed         | 3128         |\n",
      "|    total_timesteps      | 233472       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 30.526833    |\n",
      "|    clip_fraction        | 0.724        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -54.2        |\n",
      "|    explained_variance   | -0.735       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.442       |\n",
      "|    n_updates            | 1130         |\n",
      "|    policy_gradient_loss | 0.169        |\n",
      "|    reward               | 9.151802e-05 |\n",
      "|    std                  | 1.69         |\n",
      "|    value_loss           | 3.18e-06     |\n",
      "------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 135, ResetDay: 1815,Episode: 140\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "day: 1815, episode: 140\n",
      "begin_total_asset: 500.00\n",
      "end_total_asset: 423.02\n",
      "total_reward: -76.98\n",
      "total_cost: 4.95\n",
      "total_trades: 47040\n",
      "Sharpe: 0.140\n",
      "=================================\n",
      "Reseting Environment StartDay: 521, ResetDay: 2201,Episode: 141\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 115          |\n",
      "|    time_elapsed         | 3157         |\n",
      "|    total_timesteps      | 235520       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 47.59177     |\n",
      "|    clip_fraction        | 0.688        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -54.4        |\n",
      "|    explained_variance   | -0.164       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.308       |\n",
      "|    n_updates            | 1140         |\n",
      "|    policy_gradient_loss | 0.429        |\n",
      "|    reward               | -8.52345e-05 |\n",
      "|    std                  | 1.7          |\n",
      "|    value_loss           | 4.98e-06     |\n",
      "------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 522, ResetDay: 2202,Episode: 142\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 74             |\n",
      "|    iterations           | 116            |\n",
      "|    time_elapsed         | 3184           |\n",
      "|    total_timesteps      | 237568         |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 107.24578      |\n",
      "|    clip_fraction        | 0.843          |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -54.6          |\n",
      "|    explained_variance   | -1.57          |\n",
      "|    learning_rate        | 0.00025        |\n",
      "|    loss                 | -0.338         |\n",
      "|    n_updates            | 1150           |\n",
      "|    policy_gradient_loss | 0.327          |\n",
      "|    reward               | -0.00026444273 |\n",
      "|    std                  | 1.71           |\n",
      "|    value_loss           | 5.96e-06       |\n",
      "--------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 2197, ResetDay: 3877,Episode: 143\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 117           |\n",
      "|    time_elapsed         | 3212          |\n",
      "|    total_timesteps      | 239616        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 29.735844     |\n",
      "|    clip_fraction        | 0.809         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -54.7         |\n",
      "|    explained_variance   | -0.815        |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.395        |\n",
      "|    n_updates            | 1160          |\n",
      "|    policy_gradient_loss | 0.244         |\n",
      "|    reward               | 0.00028101617 |\n",
      "|    std                  | 1.72          |\n",
      "|    value_loss           | 5.34e-06      |\n",
      "-------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 715, ResetDay: 2395,Episode: 144\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 118          |\n",
      "|    time_elapsed         | 3240         |\n",
      "|    total_timesteps      | 241664       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 26.587118    |\n",
      "|    clip_fraction        | 0.747        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -54.9        |\n",
      "|    explained_variance   | -0.364       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.376       |\n",
      "|    n_updates            | 1170         |\n",
      "|    policy_gradient_loss | 0.279        |\n",
      "|    reward               | 6.944046e-05 |\n",
      "|    std                  | 1.72         |\n",
      "|    value_loss           | 3.34e-05     |\n",
      "------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 1973, ResetDay: 3653,Episode: 145\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 119           |\n",
      "|    time_elapsed         | 3267          |\n",
      "|    total_timesteps      | 243712        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 112.778015    |\n",
      "|    clip_fraction        | 0.721         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -55           |\n",
      "|    explained_variance   | -0.377        |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.366        |\n",
      "|    n_updates            | 1180          |\n",
      "|    policy_gradient_loss | 0.173         |\n",
      "|    reward               | -0.0010131553 |\n",
      "|    std                  | 1.73          |\n",
      "|    value_loss           | 3.87e-06      |\n",
      "-------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "day: 3653, episode: 145\n",
      "begin_total_asset: 500.00\n",
      "end_total_asset: 751.95\n",
      "total_reward: 251.95\n",
      "total_cost: 4.84\n",
      "total_trades: 47030\n",
      "Sharpe: 0.434\n",
      "=================================\n",
      "Reseting Environment StartDay: 1411, ResetDay: 3091,Episode: 146\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 2542, ResetDay: 4222,Episode: 147\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 120          |\n",
      "|    time_elapsed         | 3295         |\n",
      "|    total_timesteps      | 245760       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 46.99315     |\n",
      "|    clip_fraction        | 0.667        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -55.1        |\n",
      "|    explained_variance   | -0.0528      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.41        |\n",
      "|    n_updates            | 1190         |\n",
      "|    policy_gradient_loss | 0.161        |\n",
      "|    reward               | 0.0007632145 |\n",
      "|    std                  | 1.74         |\n",
      "|    value_loss           | 2.39e-06     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 2189, ResetDay: 3869,Episode: 148\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 121           |\n",
      "|    time_elapsed         | 3323          |\n",
      "|    total_timesteps      | 247808        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 174.47043     |\n",
      "|    clip_fraction        | 0.716         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -55.3         |\n",
      "|    explained_variance   | -0.28         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.405        |\n",
      "|    n_updates            | 1200          |\n",
      "|    policy_gradient_loss | 0.141         |\n",
      "|    reward               | 0.00019043083 |\n",
      "|    std                  | 1.75          |\n",
      "|    value_loss           | 1.92e-06      |\n",
      "-------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 1981, ResetDay: 3661,Episode: 149\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 74             |\n",
      "|    iterations           | 122            |\n",
      "|    time_elapsed         | 3350           |\n",
      "|    total_timesteps      | 249856         |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 75.9753        |\n",
      "|    clip_fraction        | 0.731          |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -55.4          |\n",
      "|    explained_variance   | 0.079          |\n",
      "|    learning_rate        | 0.00025        |\n",
      "|    loss                 | -0.349         |\n",
      "|    n_updates            | 1210           |\n",
      "|    policy_gradient_loss | 0.197          |\n",
      "|    reward               | -0.00018347855 |\n",
      "|    std                  | 1.76           |\n",
      "|    value_loss           | 2.84e-06       |\n",
      "--------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 2, ResetDay: 1682,Episode: 150\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 74             |\n",
      "|    iterations           | 123            |\n",
      "|    time_elapsed         | 3378           |\n",
      "|    total_timesteps      | 251904         |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 74.27241       |\n",
      "|    clip_fraction        | 0.703          |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -55.5          |\n",
      "|    explained_variance   | 0.0233         |\n",
      "|    learning_rate        | 0.00025        |\n",
      "|    loss                 | -0.238         |\n",
      "|    n_updates            | 1220           |\n",
      "|    policy_gradient_loss | 0.168          |\n",
      "|    reward               | -0.00028670655 |\n",
      "|    std                  | 1.76           |\n",
      "|    value_loss           | 1.42e-06       |\n",
      "--------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "day: 1682, episode: 150\n",
      "begin_total_asset: 500.00\n",
      "end_total_asset: 285.64\n",
      "total_reward: -214.36\n",
      "total_cost: 4.95\n",
      "total_trades: 47030\n",
      "Sharpe: -0.008\n",
      "=================================\n",
      "Reseting Environment StartDay: 2133, ResetDay: 3813,Episode: 151\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 2675, ResetDay: 4355,Episode: 152\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 74             |\n",
      "|    iterations           | 124            |\n",
      "|    time_elapsed         | 3405           |\n",
      "|    total_timesteps      | 253952         |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 97.96738       |\n",
      "|    clip_fraction        | 0.792          |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -55.6          |\n",
      "|    explained_variance   | -1.86          |\n",
      "|    learning_rate        | 0.00025        |\n",
      "|    loss                 | -0.423         |\n",
      "|    n_updates            | 1230           |\n",
      "|    policy_gradient_loss | 0.272          |\n",
      "|    reward               | -1.3656998e-05 |\n",
      "|    std                  | 1.77           |\n",
      "|    value_loss           | 7.06e-06       |\n",
      "--------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 1047, ResetDay: 2727,Episode: 153\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 74             |\n",
      "|    iterations           | 125            |\n",
      "|    time_elapsed         | 3433           |\n",
      "|    total_timesteps      | 256000         |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 34.39389       |\n",
      "|    clip_fraction        | 0.725          |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -55.8          |\n",
      "|    explained_variance   | 0.0749         |\n",
      "|    learning_rate        | 0.00025        |\n",
      "|    loss                 | -0.405         |\n",
      "|    n_updates            | 1240           |\n",
      "|    policy_gradient_loss | 0.199          |\n",
      "|    reward               | -0.00014599515 |\n",
      "|    std                  | 1.79           |\n",
      "|    value_loss           | 8.31e-06       |\n",
      "--------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 1445, ResetDay: 3125,Episode: 154\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 74             |\n",
      "|    iterations           | 126            |\n",
      "|    time_elapsed         | 3460           |\n",
      "|    total_timesteps      | 258048         |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 71.15776       |\n",
      "|    clip_fraction        | 0.706          |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -56            |\n",
      "|    explained_variance   | -0.155         |\n",
      "|    learning_rate        | 0.00025        |\n",
      "|    loss                 | -0.36          |\n",
      "|    n_updates            | 1250           |\n",
      "|    policy_gradient_loss | 0.181          |\n",
      "|    reward               | -1.4320183e-05 |\n",
      "|    std                  | 1.8            |\n",
      "|    value_loss           | 3.75e-06       |\n",
      "--------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 372, ResetDay: 2052,Episode: 155\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 74             |\n",
      "|    iterations           | 127            |\n",
      "|    time_elapsed         | 3488           |\n",
      "|    total_timesteps      | 260096         |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 34.582405      |\n",
      "|    clip_fraction        | 0.772          |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -56.1          |\n",
      "|    explained_variance   | -0.58          |\n",
      "|    learning_rate        | 0.00025        |\n",
      "|    loss                 | -0.393         |\n",
      "|    n_updates            | 1260           |\n",
      "|    policy_gradient_loss | 0.193          |\n",
      "|    reward               | -0.00013208628 |\n",
      "|    std                  | 1.8            |\n",
      "|    value_loss           | 2.11e-06       |\n",
      "--------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "day: 2052, episode: 155\n",
      "begin_total_asset: 500.00\n",
      "end_total_asset: 409.50\n",
      "total_reward: -90.50\n",
      "total_cost: 5.15\n",
      "total_trades: 47031\n",
      "Sharpe: 0.080\n",
      "=================================\n",
      "Reseting Environment StartDay: 2619, ResetDay: 4299,Episode: 156\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 128           |\n",
      "|    time_elapsed         | 3515          |\n",
      "|    total_timesteps      | 262144        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 56.01606      |\n",
      "|    clip_fraction        | 0.721         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -56.2         |\n",
      "|    explained_variance   | -1.25         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.372        |\n",
      "|    n_updates            | 1270          |\n",
      "|    policy_gradient_loss | 0.163         |\n",
      "|    reward               | -0.0013105801 |\n",
      "|    std                  | 1.81          |\n",
      "|    value_loss           | 3.54e-06      |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 2778, ResetDay: 4458,Episode: 157\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 1245, ResetDay: 2925,Episode: 158\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 129           |\n",
      "|    time_elapsed         | 3543          |\n",
      "|    total_timesteps      | 264192        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 29.378963     |\n",
      "|    clip_fraction        | 0.717         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -56.3         |\n",
      "|    explained_variance   | 0.0979        |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.384        |\n",
      "|    n_updates            | 1280          |\n",
      "|    policy_gradient_loss | 0.843         |\n",
      "|    reward               | 3.0229186e-05 |\n",
      "|    std                  | 1.82          |\n",
      "|    value_loss           | 1.52e-05      |\n",
      "-------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 875, ResetDay: 2555,Episode: 159\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 130           |\n",
      "|    time_elapsed         | 3570          |\n",
      "|    total_timesteps      | 266240        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 331.47458     |\n",
      "|    clip_fraction        | 0.695         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -56.4         |\n",
      "|    explained_variance   | -0.216        |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.429        |\n",
      "|    n_updates            | 1290          |\n",
      "|    policy_gradient_loss | 0.188         |\n",
      "|    reward               | -0.0002722253 |\n",
      "|    std                  | 1.82          |\n",
      "|    value_loss           | 3.71e-06      |\n",
      "-------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 1966, ResetDay: 3646,Episode: 160\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 131           |\n",
      "|    time_elapsed         | 3597          |\n",
      "|    total_timesteps      | 268288        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 34.44777      |\n",
      "|    clip_fraction        | 0.72          |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -56.5         |\n",
      "|    explained_variance   | -1.41         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.217        |\n",
      "|    n_updates            | 1300          |\n",
      "|    policy_gradient_loss | 0.187         |\n",
      "|    reward               | -8.100357e-05 |\n",
      "|    std                  | 1.83          |\n",
      "|    value_loss           | 1.83e-06      |\n",
      "-------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "day: 3646, episode: 160\n",
      "begin_total_asset: 500.00\n",
      "end_total_asset: 746.02\n",
      "total_reward: 246.02\n",
      "total_cost: 4.94\n",
      "total_trades: 47030\n",
      "Sharpe: 0.419\n",
      "=================================\n",
      "Reseting Environment StartDay: 746, ResetDay: 2426,Episode: 161\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 74             |\n",
      "|    iterations           | 132            |\n",
      "|    time_elapsed         | 3625           |\n",
      "|    total_timesteps      | 270336         |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 33.493668      |\n",
      "|    clip_fraction        | 0.75           |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -56.7          |\n",
      "|    explained_variance   | -0.00267       |\n",
      "|    learning_rate        | 0.00025        |\n",
      "|    loss                 | -0.419         |\n",
      "|    n_updates            | 1310           |\n",
      "|    policy_gradient_loss | 0.179          |\n",
      "|    reward               | -8.4262654e-05 |\n",
      "|    std                  | 1.85           |\n",
      "|    value_loss           | 1.52e-06       |\n",
      "--------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 1783, ResetDay: 3463,Episode: 162\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 657, ResetDay: 2337,Episode: 163\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 74             |\n",
      "|    iterations           | 133            |\n",
      "|    time_elapsed         | 3652           |\n",
      "|    total_timesteps      | 272384         |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 88.21134       |\n",
      "|    clip_fraction        | 0.728          |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -56.9          |\n",
      "|    explained_variance   | -0.449         |\n",
      "|    learning_rate        | 0.00025        |\n",
      "|    loss                 | -0.476         |\n",
      "|    n_updates            | 1320           |\n",
      "|    policy_gradient_loss | 0.237          |\n",
      "|    reward               | -0.00035041384 |\n",
      "|    std                  | 1.85           |\n",
      "|    value_loss           | 3.4e-06        |\n",
      "--------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 577, ResetDay: 2257,Episode: 164\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 134          |\n",
      "|    time_elapsed         | 3680         |\n",
      "|    total_timesteps      | 274432       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 47.10784     |\n",
      "|    clip_fraction        | 0.721        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -57          |\n",
      "|    explained_variance   | 0.0219       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.302       |\n",
      "|    n_updates            | 1330         |\n",
      "|    policy_gradient_loss | 0.169        |\n",
      "|    reward               | 0.0001220718 |\n",
      "|    std                  | 1.86         |\n",
      "|    value_loss           | 2.77e-06     |\n",
      "------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 1152, ResetDay: 2832,Episode: 165\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 74             |\n",
      "|    iterations           | 135            |\n",
      "|    time_elapsed         | 3707           |\n",
      "|    total_timesteps      | 276480         |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 27.887577      |\n",
      "|    clip_fraction        | 0.752          |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -57.1          |\n",
      "|    explained_variance   | -1.3           |\n",
      "|    learning_rate        | 0.00025        |\n",
      "|    loss                 | 1.89           |\n",
      "|    n_updates            | 1340           |\n",
      "|    policy_gradient_loss | 0.329          |\n",
      "|    reward               | -0.00018238525 |\n",
      "|    std                  | 1.87           |\n",
      "|    value_loss           | 3.1e-06        |\n",
      "--------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "day: 2832, episode: 165\n",
      "begin_total_asset: 500.00\n",
      "end_total_asset: 572.02\n",
      "total_reward: 72.02\n",
      "total_cost: 4.94\n",
      "total_trades: 47030\n",
      "Sharpe: 0.226\n",
      "=================================\n",
      "Reseting Environment StartDay: 2747, ResetDay: 4427,Episode: 166\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 136          |\n",
      "|    time_elapsed         | 3735         |\n",
      "|    total_timesteps      | 278528       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 23.836166    |\n",
      "|    clip_fraction        | 0.715        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -57.2        |\n",
      "|    explained_variance   | -0.746       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.439       |\n",
      "|    n_updates            | 1350         |\n",
      "|    policy_gradient_loss | 0.179        |\n",
      "|    reward               | 0.0008633152 |\n",
      "|    std                  | 1.88         |\n",
      "|    value_loss           | 1.83e-06     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 1430, ResetDay: 3110,Episode: 167\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 137           |\n",
      "|    time_elapsed         | 3763          |\n",
      "|    total_timesteps      | 280576        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 37.677612     |\n",
      "|    clip_fraction        | 0.707         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -57.4         |\n",
      "|    explained_variance   | 0.117         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.418        |\n",
      "|    n_updates            | 1360          |\n",
      "|    policy_gradient_loss | 0.194         |\n",
      "|    reward               | -7.900868e-05 |\n",
      "|    std                  | 1.89          |\n",
      "|    value_loss           | 3.35e-06      |\n",
      "-------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 1208, ResetDay: 2888,Episode: 168\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 2139, ResetDay: 3819,Episode: 169\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 138           |\n",
      "|    time_elapsed         | 3790          |\n",
      "|    total_timesteps      | 282624        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 182.85977     |\n",
      "|    clip_fraction        | 0.748         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -57.5         |\n",
      "|    explained_variance   | -0.0646       |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.438        |\n",
      "|    n_updates            | 1370          |\n",
      "|    policy_gradient_loss | 0.25          |\n",
      "|    reward               | 0.00030093917 |\n",
      "|    std                  | 1.89          |\n",
      "|    value_loss           | 1.06e-05      |\n",
      "-------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 991, ResetDay: 2671,Episode: 170\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 139           |\n",
      "|    time_elapsed         | 3817          |\n",
      "|    total_timesteps      | 284672        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 81.60398      |\n",
      "|    clip_fraction        | 0.693         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -57.7         |\n",
      "|    explained_variance   | -0.225        |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.419        |\n",
      "|    n_updates            | 1380          |\n",
      "|    policy_gradient_loss | 0.167         |\n",
      "|    reward               | 0.00020930033 |\n",
      "|    std                  | 1.91          |\n",
      "|    value_loss           | 1.71e-06      |\n",
      "-------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "day: 2671, episode: 170\n",
      "begin_total_asset: 500.00\n",
      "end_total_asset: 290.19\n",
      "total_reward: -209.81\n",
      "total_cost: 5.15\n",
      "total_trades: 47031\n",
      "Sharpe: -0.004\n",
      "=================================\n",
      "Reseting Environment StartDay: 1144, ResetDay: 2824,Episode: 171\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 74             |\n",
      "|    iterations           | 140            |\n",
      "|    time_elapsed         | 3845           |\n",
      "|    total_timesteps      | 286720         |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 54.53232       |\n",
      "|    clip_fraction        | 0.802          |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -57.8          |\n",
      "|    explained_variance   | 0.06           |\n",
      "|    learning_rate        | 0.00025        |\n",
      "|    loss                 | -0.441         |\n",
      "|    n_updates            | 1390           |\n",
      "|    policy_gradient_loss | 0.226          |\n",
      "|    reward               | -4.7141264e-05 |\n",
      "|    std                  | 1.92           |\n",
      "|    value_loss           | 2.77e-06       |\n",
      "--------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 1267, ResetDay: 2947,Episode: 172\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 141           |\n",
      "|    time_elapsed         | 3872          |\n",
      "|    total_timesteps      | 288768        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 32.88099      |\n",
      "|    clip_fraction        | 0.716         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -58           |\n",
      "|    explained_variance   | -0.0439       |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.468        |\n",
      "|    n_updates            | 1400          |\n",
      "|    policy_gradient_loss | 0.289         |\n",
      "|    reward               | -0.0005799847 |\n",
      "|    std                  | 1.92          |\n",
      "|    value_loss           | 1.91e-06      |\n",
      "-------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 1332, ResetDay: 3012,Episode: 173\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 2742, ResetDay: 4422,Episode: 174\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 142           |\n",
      "|    time_elapsed         | 3900          |\n",
      "|    total_timesteps      | 290816        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 44.856865     |\n",
      "|    clip_fraction        | 0.762         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -58.1         |\n",
      "|    explained_variance   | 0.113         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.535        |\n",
      "|    n_updates            | 1410          |\n",
      "|    policy_gradient_loss | 0.283         |\n",
      "|    reward               | -0.0007323593 |\n",
      "|    std                  | 1.94          |\n",
      "|    value_loss           | 2.56e-06      |\n",
      "-------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 1314, ResetDay: 2994,Episode: 175\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 143           |\n",
      "|    time_elapsed         | 3928          |\n",
      "|    total_timesteps      | 292864        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 63.158577     |\n",
      "|    clip_fraction        | 0.702         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -58.2         |\n",
      "|    explained_variance   | 0.103         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.467        |\n",
      "|    n_updates            | 1420          |\n",
      "|    policy_gradient_loss | 0.204         |\n",
      "|    reward               | 4.4059183e-05 |\n",
      "|    std                  | 1.94          |\n",
      "|    value_loss           | 2.24e-06      |\n",
      "-------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "day: 2994, episode: 175\n",
      "begin_total_asset: 500.00\n",
      "end_total_asset: 360.55\n",
      "total_reward: -139.45\n",
      "total_cost: 5.18\n",
      "total_trades: 47031\n",
      "Sharpe: 0.094\n",
      "=================================\n",
      "Reseting Environment StartDay: 1068, ResetDay: 2748,Episode: 176\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 144          |\n",
      "|    time_elapsed         | 3955         |\n",
      "|    total_timesteps      | 294912       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 60.390167    |\n",
      "|    clip_fraction        | 0.738        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -58.3        |\n",
      "|    explained_variance   | 0.0499       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.356       |\n",
      "|    n_updates            | 1430         |\n",
      "|    policy_gradient_loss | 0.179        |\n",
      "|    reward               | 8.157339e-05 |\n",
      "|    std                  | 1.95         |\n",
      "|    value_loss           | 4.95e-06     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 2609, ResetDay: 4289,Episode: 177\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 74             |\n",
      "|    iterations           | 145            |\n",
      "|    time_elapsed         | 3983           |\n",
      "|    total_timesteps      | 296960         |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 31.900352      |\n",
      "|    clip_fraction        | 0.746          |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -58.4          |\n",
      "|    explained_variance   | -0.837         |\n",
      "|    learning_rate        | 0.00025        |\n",
      "|    loss                 | -0.487         |\n",
      "|    n_updates            | 1440           |\n",
      "|    policy_gradient_loss | 0.186          |\n",
      "|    reward               | -0.00022041054 |\n",
      "|    std                  | 1.96           |\n",
      "|    value_loss           | 1.76e-06       |\n",
      "--------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 2310, ResetDay: 3990,Episode: 178\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 146           |\n",
      "|    time_elapsed         | 4011          |\n",
      "|    total_timesteps      | 299008        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 31.99477      |\n",
      "|    clip_fraction        | 0.688         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -58.6         |\n",
      "|    explained_variance   | 0.105         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.419        |\n",
      "|    n_updates            | 1450          |\n",
      "|    policy_gradient_loss | 0.192         |\n",
      "|    reward               | 5.7974245e-05 |\n",
      "|    std                  | 1.97          |\n",
      "|    value_loss           | 2.14e-06      |\n",
      "-------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 1610, ResetDay: 3290,Episode: 179\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 356, ResetDay: 2036,Episode: 180\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 147          |\n",
      "|    time_elapsed         | 4038         |\n",
      "|    total_timesteps      | 301056       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 163.12389    |\n",
      "|    clip_fraction        | 0.737        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -58.7        |\n",
      "|    explained_variance   | 0.0576       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.408       |\n",
      "|    n_updates            | 1460         |\n",
      "|    policy_gradient_loss | 0.24         |\n",
      "|    reward               | 2.568035e-05 |\n",
      "|    std                  | 1.98         |\n",
      "|    value_loss           | 7.08e-06     |\n",
      "------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "day: 2036, episode: 180\n",
      "begin_total_asset: 500.00\n",
      "end_total_asset: 447.35\n",
      "total_reward: -52.65\n",
      "total_cost: 5.57\n",
      "total_trades: 47037\n",
      "Sharpe: 0.138\n",
      "=================================\n",
      "Reseting Environment StartDay: 2491, ResetDay: 4171,Episode: 181\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 74             |\n",
      "|    iterations           | 148            |\n",
      "|    time_elapsed         | 4065           |\n",
      "|    total_timesteps      | 303104         |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 109.01665      |\n",
      "|    clip_fraction        | 0.755          |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -58.9          |\n",
      "|    explained_variance   | -0.272         |\n",
      "|    learning_rate        | 0.00025        |\n",
      "|    loss                 | -0.0748        |\n",
      "|    n_updates            | 1470           |\n",
      "|    policy_gradient_loss | 0.201          |\n",
      "|    reward               | -0.00021423073 |\n",
      "|    std                  | 1.99           |\n",
      "|    value_loss           | 8.29e-06       |\n",
      "--------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 1964, ResetDay: 3644,Episode: 182\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 149           |\n",
      "|    time_elapsed         | 4093          |\n",
      "|    total_timesteps      | 305152        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 31.991714     |\n",
      "|    clip_fraction        | 0.73          |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -59           |\n",
      "|    explained_variance   | -0.967        |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.42         |\n",
      "|    n_updates            | 1480          |\n",
      "|    policy_gradient_loss | 0.232         |\n",
      "|    reward               | 2.0240784e-06 |\n",
      "|    std                  | 2             |\n",
      "|    value_loss           | 2.73e-06      |\n",
      "-------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 2782, ResetDay: 4462,Episode: 183\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 150          |\n",
      "|    time_elapsed         | 4120         |\n",
      "|    total_timesteps      | 307200       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 81.60605     |\n",
      "|    clip_fraction        | 0.721        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -59.1        |\n",
      "|    explained_variance   | 0.119        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.401       |\n",
      "|    n_updates            | 1490         |\n",
      "|    policy_gradient_loss | 0.387        |\n",
      "|    reward               | 0.0003791275 |\n",
      "|    std                  | 2.01         |\n",
      "|    value_loss           | 3.41e-06     |\n",
      "------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 1462, ResetDay: 3142,Episode: 184\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 151           |\n",
      "|    time_elapsed         | 4148          |\n",
      "|    total_timesteps      | 309248        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 66.71037      |\n",
      "|    clip_fraction        | 0.753         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -59.2         |\n",
      "|    explained_variance   | 0.147         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.353        |\n",
      "|    n_updates            | 1500          |\n",
      "|    policy_gradient_loss | 0.392         |\n",
      "|    reward               | -6.734505e-05 |\n",
      "|    std                  | 2.02          |\n",
      "|    value_loss           | 6.27e-06      |\n",
      "-------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 1712, ResetDay: 3392,Episode: 185\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "day: 3392, episode: 185\n",
      "begin_total_asset: 500.00\n",
      "end_total_asset: 572.83\n",
      "total_reward: 72.83\n",
      "total_cost: 4.89\n",
      "total_trades: 47040\n",
      "Sharpe: 0.227\n",
      "=================================\n",
      "Reseting Environment StartDay: 1940, ResetDay: 3620,Episode: 186\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 152          |\n",
      "|    time_elapsed         | 4175         |\n",
      "|    total_timesteps      | 311296       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 227.5489     |\n",
      "|    clip_fraction        | 0.767        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -59.3        |\n",
      "|    explained_variance   | -0.215       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.405       |\n",
      "|    n_updates            | 1510         |\n",
      "|    policy_gradient_loss | 0.229        |\n",
      "|    reward               | 0.0002677307 |\n",
      "|    std                  | 2.02         |\n",
      "|    value_loss           | 2.45e-06     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 253, ResetDay: 1933,Episode: 187\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 74             |\n",
      "|    iterations           | 153            |\n",
      "|    time_elapsed         | 4203           |\n",
      "|    total_timesteps      | 313344         |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 91.07977       |\n",
      "|    clip_fraction        | 0.758          |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -59.4          |\n",
      "|    explained_variance   | 0.0622         |\n",
      "|    learning_rate        | 0.00025        |\n",
      "|    loss                 | -0.438         |\n",
      "|    n_updates            | 1520           |\n",
      "|    policy_gradient_loss | 0.762          |\n",
      "|    reward               | -7.0860675e-05 |\n",
      "|    std                  | 2.03           |\n",
      "|    value_loss           | 2.03e-06       |\n",
      "--------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 221, ResetDay: 1901,Episode: 188\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 154           |\n",
      "|    time_elapsed         | 4230          |\n",
      "|    total_timesteps      | 315392        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 47.994926     |\n",
      "|    clip_fraction        | 0.682         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -59.5         |\n",
      "|    explained_variance   | -0.442        |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.427        |\n",
      "|    n_updates            | 1530          |\n",
      "|    policy_gradient_loss | 0.138         |\n",
      "|    reward               | -9.421129e-05 |\n",
      "|    std                  | 2.04          |\n",
      "|    value_loss           | 3.13e-06      |\n",
      "-------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 1073, ResetDay: 2753,Episode: 189\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 74             |\n",
      "|    iterations           | 155            |\n",
      "|    time_elapsed         | 4258           |\n",
      "|    total_timesteps      | 317440         |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 21.948528      |\n",
      "|    clip_fraction        | 0.736          |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -59.6          |\n",
      "|    explained_variance   | -1.71          |\n",
      "|    learning_rate        | 0.00025        |\n",
      "|    loss                 | -0.39          |\n",
      "|    n_updates            | 1540           |\n",
      "|    policy_gradient_loss | 0.229          |\n",
      "|    reward               | -0.00059574697 |\n",
      "|    std                  | 2.05           |\n",
      "|    value_loss           | 3e-06          |\n",
      "--------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 2574, ResetDay: 4254,Episode: 190\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "day: 4254, episode: 190\n",
      "begin_total_asset: 500.00\n",
      "end_total_asset: 800.74\n",
      "total_reward: 300.74\n",
      "total_cost: 4.89\n",
      "total_trades: 47040\n",
      "Sharpe: 0.454\n",
      "=================================\n",
      "Reseting Environment StartDay: 1950, ResetDay: 3630,Episode: 191\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 156           |\n",
      "|    time_elapsed         | 4285          |\n",
      "|    total_timesteps      | 319488        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 30.343838     |\n",
      "|    clip_fraction        | 0.751         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -59.8         |\n",
      "|    explained_variance   | -0.266        |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.503        |\n",
      "|    n_updates            | 1550          |\n",
      "|    policy_gradient_loss | 0.257         |\n",
      "|    reward               | 5.1999283e-05 |\n",
      "|    std                  | 2.05          |\n",
      "|    value_loss           | 2.46e-06      |\n",
      "-------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 1372, ResetDay: 3052,Episode: 192\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 157           |\n",
      "|    time_elapsed         | 4313          |\n",
      "|    total_timesteps      | 321536        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 58.540802     |\n",
      "|    clip_fraction        | 0.761         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -59.9         |\n",
      "|    explained_variance   | 0.0547        |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.347        |\n",
      "|    n_updates            | 1560          |\n",
      "|    policy_gradient_loss | 0.242         |\n",
      "|    reward               | 5.5600547e-05 |\n",
      "|    std                  | 2.07          |\n",
      "|    value_loss           | 4.11e-06      |\n",
      "-------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 1528, ResetDay: 3208,Episode: 193\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 74             |\n",
      "|    iterations           | 158            |\n",
      "|    time_elapsed         | 4340           |\n",
      "|    total_timesteps      | 323584         |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 42.367043      |\n",
      "|    clip_fraction        | 0.731          |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -60.1          |\n",
      "|    explained_variance   | -0.0196        |\n",
      "|    learning_rate        | 0.00025        |\n",
      "|    loss                 | -0.461         |\n",
      "|    n_updates            | 1570           |\n",
      "|    policy_gradient_loss | 0.307          |\n",
      "|    reward               | -0.00029055213 |\n",
      "|    std                  | 2.08           |\n",
      "|    value_loss           | 1.59e-06       |\n",
      "--------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 1129, ResetDay: 2809,Episode: 194\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 159           |\n",
      "|    time_elapsed         | 4368          |\n",
      "|    total_timesteps      | 325632        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 34.494656     |\n",
      "|    clip_fraction        | 0.77          |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -60.3         |\n",
      "|    explained_variance   | 0.0616        |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.344        |\n",
      "|    n_updates            | 1580          |\n",
      "|    policy_gradient_loss | 0.315         |\n",
      "|    reward               | 0.00011786919 |\n",
      "|    std                  | 2.09          |\n",
      "|    value_loss           | 1.3e-06       |\n",
      "-------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 2392, ResetDay: 4072,Episode: 195\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 74             |\n",
      "|    iterations           | 160            |\n",
      "|    time_elapsed         | 4396           |\n",
      "|    total_timesteps      | 327680         |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 52.097385      |\n",
      "|    clip_fraction        | 0.775          |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -60.4          |\n",
      "|    explained_variance   | -0.044         |\n",
      "|    learning_rate        | 0.00025        |\n",
      "|    loss                 | -0.348         |\n",
      "|    n_updates            | 1590           |\n",
      "|    policy_gradient_loss | 0.397          |\n",
      "|    reward               | -0.00025016099 |\n",
      "|    std                  | 2.1            |\n",
      "|    value_loss           | 4.54e-06       |\n",
      "--------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "day: 4072, episode: 195\n",
      "begin_total_asset: 500.00\n",
      "end_total_asset: 958.13\n",
      "total_reward: 458.13\n",
      "total_cost: 4.93\n",
      "total_trades: 47031\n",
      "Sharpe: 0.606\n",
      "=================================\n",
      "Reseting Environment StartDay: 182, ResetDay: 1862,Episode: 196\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 2638, ResetDay: 4318,Episode: 197\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 161           |\n",
      "|    time_elapsed         | 4423          |\n",
      "|    total_timesteps      | 329728        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 50.10978      |\n",
      "|    clip_fraction        | 0.732         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -60.5         |\n",
      "|    explained_variance   | 0.204         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.322        |\n",
      "|    n_updates            | 1600          |\n",
      "|    policy_gradient_loss | 0.171         |\n",
      "|    reward               | 0.00092973746 |\n",
      "|    std                  | 2.11          |\n",
      "|    value_loss           | 2.99e-06      |\n",
      "-------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 2651, ResetDay: 4331,Episode: 198\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 162           |\n",
      "|    time_elapsed         | 4451          |\n",
      "|    total_timesteps      | 331776        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 172.46968     |\n",
      "|    clip_fraction        | 0.702         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -60.6         |\n",
      "|    explained_variance   | -0.945        |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.499        |\n",
      "|    n_updates            | 1610          |\n",
      "|    policy_gradient_loss | 0.203         |\n",
      "|    reward               | 0.00063161924 |\n",
      "|    std                  | 2.12          |\n",
      "|    value_loss           | 3.81e-06      |\n",
      "-------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 2172, ResetDay: 3852,Episode: 199\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 74             |\n",
      "|    iterations           | 163            |\n",
      "|    time_elapsed         | 4478           |\n",
      "|    total_timesteps      | 333824         |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 60.10659       |\n",
      "|    clip_fraction        | 0.731          |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -60.7          |\n",
      "|    explained_variance   | 0.0839         |\n",
      "|    learning_rate        | 0.00025        |\n",
      "|    loss                 | -0.387         |\n",
      "|    n_updates            | 1620           |\n",
      "|    policy_gradient_loss | 0.202          |\n",
      "|    reward               | -0.00031359578 |\n",
      "|    std                  | 2.13           |\n",
      "|    value_loss           | 7.21e-06       |\n",
      "--------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 2044, ResetDay: 3724,Episode: 200\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 164          |\n",
      "|    time_elapsed         | 4506         |\n",
      "|    total_timesteps      | 335872       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 103.29926    |\n",
      "|    clip_fraction        | 0.717        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -60.9        |\n",
      "|    explained_variance   | -0.0876      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.508       |\n",
      "|    n_updates            | 1630         |\n",
      "|    policy_gradient_loss | 0.161        |\n",
      "|    reward               | 8.571968e-05 |\n",
      "|    std                  | 2.14         |\n",
      "|    value_loss           | 2.82e-06     |\n",
      "------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "day: 3724, episode: 200\n",
      "begin_total_asset: 500.00\n",
      "end_total_asset: 479.22\n",
      "total_reward: -20.78\n",
      "total_cost: 4.89\n",
      "total_trades: 47031\n",
      "Sharpe: 0.102\n",
      "=================================\n",
      "Reseting Environment StartDay: 1728, ResetDay: 3408,Episode: 201\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 2586, ResetDay: 4266,Episode: 202\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 165           |\n",
      "|    time_elapsed         | 4534          |\n",
      "|    total_timesteps      | 337920        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 93.659355     |\n",
      "|    clip_fraction        | 0.728         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -61.1         |\n",
      "|    explained_variance   | 0.18          |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.451        |\n",
      "|    n_updates            | 1640          |\n",
      "|    policy_gradient_loss | 0.18          |\n",
      "|    reward               | 0.00019976559 |\n",
      "|    std                  | 2.15          |\n",
      "|    value_loss           | 1.6e-06       |\n",
      "-------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 2594, ResetDay: 4274,Episode: 203\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 166          |\n",
      "|    time_elapsed         | 4561         |\n",
      "|    total_timesteps      | 339968       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 117.7409     |\n",
      "|    clip_fraction        | 0.702        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -61.2        |\n",
      "|    explained_variance   | 0.169        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.43        |\n",
      "|    n_updates            | 1650         |\n",
      "|    policy_gradient_loss | 0.152        |\n",
      "|    reward               | 7.615261e-05 |\n",
      "|    std                  | 2.16         |\n",
      "|    value_loss           | 2.71e-06     |\n",
      "------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 2205, ResetDay: 3885,Episode: 204\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 167          |\n",
      "|    time_elapsed         | 4589         |\n",
      "|    total_timesteps      | 342016       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 51.973892    |\n",
      "|    clip_fraction        | 0.698        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -61.3        |\n",
      "|    explained_variance   | 0.102        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.466       |\n",
      "|    n_updates            | 1660         |\n",
      "|    policy_gradient_loss | 0.183        |\n",
      "|    reward               | 0.0001887619 |\n",
      "|    std                  | 2.17         |\n",
      "|    value_loss           | 5.56e-06     |\n",
      "------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 924, ResetDay: 2604,Episode: 205\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 168           |\n",
      "|    time_elapsed         | 4617          |\n",
      "|    total_timesteps      | 344064        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 74.26323      |\n",
      "|    clip_fraction        | 0.687         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -61.5         |\n",
      "|    explained_variance   | 0.139         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.517        |\n",
      "|    n_updates            | 1670          |\n",
      "|    policy_gradient_loss | 0.16          |\n",
      "|    reward               | 0.00019326896 |\n",
      "|    std                  | 2.18          |\n",
      "|    value_loss           | 1.82e-06      |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "day: 2604, episode: 205\n",
      "begin_total_asset: 500.00\n",
      "end_total_asset: 486.73\n",
      "total_reward: -13.27\n",
      "total_cost: 4.95\n",
      "total_trades: 47031\n",
      "Sharpe: 0.248\n",
      "=================================\n",
      "Reseting Environment StartDay: 2171, ResetDay: 3851,Episode: 206\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 74             |\n",
      "|    iterations           | 169            |\n",
      "|    time_elapsed         | 4644           |\n",
      "|    total_timesteps      | 346112         |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 70.555405      |\n",
      "|    clip_fraction        | 0.691          |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -61.6          |\n",
      "|    explained_variance   | -0.187         |\n",
      "|    learning_rate        | 0.00025        |\n",
      "|    loss                 | -0.426         |\n",
      "|    n_updates            | 1680           |\n",
      "|    policy_gradient_loss | 0.232          |\n",
      "|    reward               | -0.00023931694 |\n",
      "|    std                  | 2.19           |\n",
      "|    value_loss           | 2.65e-06       |\n",
      "--------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 2255, ResetDay: 3935,Episode: 207\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 1924, ResetDay: 3604,Episode: 208\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 170           |\n",
      "|    time_elapsed         | 4672          |\n",
      "|    total_timesteps      | 348160        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 39.13682      |\n",
      "|    clip_fraction        | 0.694         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -61.7         |\n",
      "|    explained_variance   | 0.136         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.491        |\n",
      "|    n_updates            | 1690          |\n",
      "|    policy_gradient_loss | 0.172         |\n",
      "|    reward               | -0.0001334011 |\n",
      "|    std                  | 2.21          |\n",
      "|    value_loss           | 1.56e-06      |\n",
      "-------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 1842, ResetDay: 3522,Episode: 209\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 171           |\n",
      "|    time_elapsed         | 4699          |\n",
      "|    total_timesteps      | 350208        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 129.73247     |\n",
      "|    clip_fraction        | 0.776         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -61.8         |\n",
      "|    explained_variance   | 0.119         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 0.255         |\n",
      "|    n_updates            | 1700          |\n",
      "|    policy_gradient_loss | 0.311         |\n",
      "|    reward               | -0.0009176813 |\n",
      "|    std                  | 2.21          |\n",
      "|    value_loss           | 8.07e-06      |\n",
      "-------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 866, ResetDay: 2546,Episode: 210\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 74             |\n",
      "|    iterations           | 172            |\n",
      "|    time_elapsed         | 4727           |\n",
      "|    total_timesteps      | 352256         |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 40.569298      |\n",
      "|    clip_fraction        | 0.691          |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -61.9          |\n",
      "|    explained_variance   | 0.0162         |\n",
      "|    learning_rate        | 0.00025        |\n",
      "|    loss                 | -0.35          |\n",
      "|    n_updates            | 1710           |\n",
      "|    policy_gradient_loss | 0.176          |\n",
      "|    reward               | -0.00018457527 |\n",
      "|    std                  | 2.22           |\n",
      "|    value_loss           | 2.16e-06       |\n",
      "--------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "day: 2546, episode: 210\n",
      "begin_total_asset: 500.00\n",
      "end_total_asset: 445.77\n",
      "total_reward: -54.23\n",
      "total_cost: 4.95\n",
      "total_trades: 47031\n",
      "Sharpe: 0.096\n",
      "=================================\n",
      "Reseting Environment StartDay: 2451, ResetDay: 4131,Episode: 211\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 173           |\n",
      "|    time_elapsed         | 4754          |\n",
      "|    total_timesteps      | 354304        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 48.48742      |\n",
      "|    clip_fraction        | 0.725         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -62           |\n",
      "|    explained_variance   | -0.0901       |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.399        |\n",
      "|    n_updates            | 1720          |\n",
      "|    policy_gradient_loss | 0.251         |\n",
      "|    reward               | 0.00070130767 |\n",
      "|    std                  | 2.23          |\n",
      "|    value_loss           | 1.94e-06      |\n",
      "-------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 2799, ResetDay: 4479,Episode: 212\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 174          |\n",
      "|    time_elapsed         | 4782         |\n",
      "|    total_timesteps      | 356352       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 32.099228    |\n",
      "|    clip_fraction        | 0.691        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -62.2        |\n",
      "|    explained_variance   | 0.252        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.356       |\n",
      "|    n_updates            | 1730         |\n",
      "|    policy_gradient_loss | 0.19         |\n",
      "|    reward               | -0.000830373 |\n",
      "|    std                  | 2.24         |\n",
      "|    value_loss           | 1.68e-06     |\n",
      "------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 382, ResetDay: 2062,Episode: 213\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 1586, ResetDay: 3266,Episode: 214\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 175           |\n",
      "|    time_elapsed         | 4810          |\n",
      "|    total_timesteps      | 358400        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 100.20996     |\n",
      "|    clip_fraction        | 0.699         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -62.3         |\n",
      "|    explained_variance   | 0.0811        |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.377        |\n",
      "|    n_updates            | 1740          |\n",
      "|    policy_gradient_loss | 0.248         |\n",
      "|    reward               | 0.00030689908 |\n",
      "|    std                  | 2.25          |\n",
      "|    value_loss           | 2.48e-06      |\n",
      "-------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 2613, ResetDay: 4293,Episode: 215\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 176           |\n",
      "|    time_elapsed         | 4837          |\n",
      "|    total_timesteps      | 360448        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 165.39801     |\n",
      "|    clip_fraction        | 0.714         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -62.5         |\n",
      "|    explained_variance   | -0.817        |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.532        |\n",
      "|    n_updates            | 1750          |\n",
      "|    policy_gradient_loss | 0.167         |\n",
      "|    reward               | 0.00028634796 |\n",
      "|    std                  | 2.27          |\n",
      "|    value_loss           | 2.87e-06      |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "day: 4293, episode: 215\n",
      "begin_total_asset: 500.00\n",
      "end_total_asset: 888.37\n",
      "total_reward: 388.37\n",
      "total_cost: 4.92\n",
      "total_trades: 47031\n",
      "Sharpe: 0.509\n",
      "=================================\n",
      "Reseting Environment StartDay: 1447, ResetDay: 3127,Episode: 216\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 74             |\n",
      "|    iterations           | 177            |\n",
      "|    time_elapsed         | 4865           |\n",
      "|    total_timesteps      | 362496         |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 34.232132      |\n",
      "|    clip_fraction        | 0.704          |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -62.7          |\n",
      "|    explained_variance   | 0.156          |\n",
      "|    learning_rate        | 0.00025        |\n",
      "|    loss                 | -0.471         |\n",
      "|    n_updates            | 1760           |\n",
      "|    policy_gradient_loss | 0.182          |\n",
      "|    reward               | -0.00039850158 |\n",
      "|    std                  | 2.28           |\n",
      "|    value_loss           | 2.74e-06       |\n",
      "--------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 212, ResetDay: 1892,Episode: 217\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 74             |\n",
      "|    iterations           | 178            |\n",
      "|    time_elapsed         | 4892           |\n",
      "|    total_timesteps      | 364544         |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 97.92678       |\n",
      "|    clip_fraction        | 0.694          |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -62.8          |\n",
      "|    explained_variance   | -0.0987        |\n",
      "|    learning_rate        | 0.00025        |\n",
      "|    loss                 | -0.419         |\n",
      "|    n_updates            | 1770           |\n",
      "|    policy_gradient_loss | 0.219          |\n",
      "|    reward               | -0.00038527793 |\n",
      "|    std                  | 2.3            |\n",
      "|    value_loss           | 3.13e-06       |\n",
      "--------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 2674, ResetDay: 4354,Episode: 218\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 2450, ResetDay: 4130,Episode: 219\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 74             |\n",
      "|    iterations           | 179            |\n",
      "|    time_elapsed         | 4920           |\n",
      "|    total_timesteps      | 366592         |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 52.57714       |\n",
      "|    clip_fraction        | 0.775          |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -63            |\n",
      "|    explained_variance   | -1.12          |\n",
      "|    learning_rate        | 0.00025        |\n",
      "|    loss                 | -0.452         |\n",
      "|    n_updates            | 1780           |\n",
      "|    policy_gradient_loss | 0.227          |\n",
      "|    reward               | -0.00042645837 |\n",
      "|    std                  | 2.31           |\n",
      "|    value_loss           | 2.22e-06       |\n",
      "--------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 738, ResetDay: 2418,Episode: 220\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 180           |\n",
      "|    time_elapsed         | 4947          |\n",
      "|    total_timesteps      | 368640        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 30.133915     |\n",
      "|    clip_fraction        | 0.723         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -63.1         |\n",
      "|    explained_variance   | 0.0197        |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.569        |\n",
      "|    n_updates            | 1790          |\n",
      "|    policy_gradient_loss | 0.444         |\n",
      "|    reward               | 5.3948545e-05 |\n",
      "|    std                  | 2.32          |\n",
      "|    value_loss           | 6.47e-06      |\n",
      "-------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "day: 2418, episode: 220\n",
      "begin_total_asset: 500.00\n",
      "end_total_asset: 316.98\n",
      "total_reward: -183.02\n",
      "total_cost: 5.29\n",
      "total_trades: 47034\n",
      "Sharpe: 0.085\n",
      "=================================\n",
      "Reseting Environment StartDay: 1, ResetDay: 1681,Episode: 221\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 74             |\n",
      "|    iterations           | 181            |\n",
      "|    time_elapsed         | 4975           |\n",
      "|    total_timesteps      | 370688         |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 52.64708       |\n",
      "|    clip_fraction        | 0.77           |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -63.3          |\n",
      "|    explained_variance   | -0.177         |\n",
      "|    learning_rate        | 0.00025        |\n",
      "|    loss                 | -0.461         |\n",
      "|    n_updates            | 1800           |\n",
      "|    policy_gradient_loss | 0.215          |\n",
      "|    reward               | -0.00048074962 |\n",
      "|    std                  | 2.33           |\n",
      "|    value_loss           | 1.06e-05       |\n",
      "--------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 1366, ResetDay: 3046,Episode: 222\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 182          |\n",
      "|    time_elapsed         | 5003         |\n",
      "|    total_timesteps      | 372736       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 28.314487    |\n",
      "|    clip_fraction        | 0.747        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -63.5        |\n",
      "|    explained_variance   | -2.77        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.498       |\n",
      "|    n_updates            | 1810         |\n",
      "|    policy_gradient_loss | 0.17         |\n",
      "|    reward               | 0.0011481127 |\n",
      "|    std                  | 2.35         |\n",
      "|    value_loss           | 4.9e-06      |\n",
      "------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 2707, ResetDay: 4387,Episode: 223\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 74             |\n",
      "|    iterations           | 183            |\n",
      "|    time_elapsed         | 5030           |\n",
      "|    total_timesteps      | 374784         |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 24.056835      |\n",
      "|    clip_fraction        | 0.689          |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -63.6          |\n",
      "|    explained_variance   | -0.133         |\n",
      "|    learning_rate        | 0.00025        |\n",
      "|    loss                 | -0.505         |\n",
      "|    n_updates            | 1820           |\n",
      "|    policy_gradient_loss | 0.22           |\n",
      "|    reward               | -0.00010564117 |\n",
      "|    std                  | 2.36           |\n",
      "|    value_loss           | 2.71e-06       |\n",
      "--------------------------------------------\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 1450, ResetDay: 3130,Episode: 224\n",
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "Reseting Environment StartDay: 6, ResetDay: 1686,Episode: 225\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 184           |\n",
      "|    time_elapsed         | 5058          |\n",
      "|    total_timesteps      | 376832        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 49.434135     |\n",
      "|    clip_fraction        | 0.707         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -63.8         |\n",
      "|    explained_variance   | 0.0985        |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.488        |\n",
      "|    n_updates            | 1830          |\n",
      "|    policy_gradient_loss | 0.197         |\n",
      "|    reward               | 0.00018621996 |\n",
      "|    std                  | 2.38          |\n",
      "|    value_loss           | 3.54e-06      |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment reached Terminal state as number of trading days reached limit!!\n",
      "day: 1686, episode: 225\n",
      "begin_total_asset: 500.00\n",
      "end_total_asset: 362.07\n",
      "total_reward: -137.93\n",
      "total_cost: 5.37\n",
      "total_trades: 47037\n",
      "Sharpe: 0.026\n",
      "=================================\n",
      "Reseting Environment StartDay: 2173, ResetDay: 3853,Episode: 226\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 185           |\n",
      "|    time_elapsed         | 5085          |\n",
      "|    total_timesteps      | 378880        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 170.30782     |\n",
      "|    clip_fraction        | 0.733         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -63.9         |\n",
      "|    explained_variance   | -0.316        |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.583        |\n",
      "|    n_updates            | 1840          |\n",
      "|    policy_gradient_loss | 0.185         |\n",
      "|    reward               | -0.0029273473 |\n",
      "|    std                  | 2.38          |\n",
      "|    value_loss           | 3.3e-06       |\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "status = agent.run_strategy(A2C_model_kwargs,\n",
    "                             PPO_model_kwargs,\n",
    "                             DDPG_model_kwargs,\n",
    "                             timesteps_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
